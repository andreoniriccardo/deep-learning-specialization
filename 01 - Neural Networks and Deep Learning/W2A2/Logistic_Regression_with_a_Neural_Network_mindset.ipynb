{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with a Neural Network mindset\n",
    "\n",
    "Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.\n",
    "\n",
    "**Instructions:**\n",
    "- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n",
    "- Use `np.dot(X,Y)` to calculate dot products.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order.\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/neural-networks-deep-learning/supplement/iLwon/h-ow-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Overview of the Problem set](#2)\n",
    "    - [Exercise 1](#ex-1)\n",
    "    - [Exercise 2](#ex-2)\n",
    "- [3 - General Architecture of the learning algorithm](#3)\n",
    "- [4 - Building the parts of our algorithm](#4)\n",
    "    - [4.1 - Helper functions](#4-1)\n",
    "        - [Exercise 3 - sigmoid](#ex-3)\n",
    "    - [4.2 - Initializing parameters](#4-2)\n",
    "        - [Exercise 4 - initialize_with_zeros](#ex-4)\n",
    "    - [4.3 - Forward and Backward propagation](#4-3)\n",
    "        - [Exercise 5 - propagate](#ex-5)\n",
    "    - [4.4 - Optimization](#4-4)\n",
    "        - [Exercise 6 - optimize](#ex-6)\n",
    "        - [Exercise 7 - predict](#ex-7)\n",
    "- [5 - Merge all functions into a model](#5)\n",
    "    - [Exercise 8 - model](#ex-8)\n",
    "- [6 - Further analysis (optional/ungraded exercise)](#6)\n",
    "- [7 - Test with your own image (optional/ungraded exercise)](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](https://numpy.org/doc/1.20/) is the fundamental package for scientific computing with Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.\n",
    "- [PIL](https://pillow.readthedocs.io/en/stable/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Overview of the Problem set ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset (\"data.h5\") containing:\n",
    "    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - a test set of m_test images labeled as cat or non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).\n",
    "\n",
    "You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added \"_orig\" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).\n",
    "\n",
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [1], it's a 'cat' picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO19aYxk13XeOa/26uq9Z3o2coabJUqUSUm0TIlMQG0OvcT6pcQGHCiJAMKGE8iIA0tKgAAOEEBBAMP5YSQg4kWIbTmCbUWC4I1gRDuOZUrUQomLyCGHPTM909M903vX/l7d/OjqOt851VVTM9NTTbvOBzT6vrr33XfrvnfrnXPPOd/hEAI5HI6//4gOewAOh2M48MXucIwIfLE7HCMCX+wOx4jAF7vDMSLwxe5wjAhuabEz8xPM/Cozv87MnzmoQTkcjoMH36ydnZlTRPQaEX2UiBaJ6JtE9LMhhJcPbngOh+OgkL6Fc99HRK+HEM4RETHzHxDRx4io52KPIg5RxNft2P7+6GM5P4pSql0qheWMqmu1kn3LIbTMteRizHqsqfR4p9yM89Cf/cFsQoexqomiBMq6jknGon+Eg2k3GNRZdlJ5sF5wHF2vhbBvset6UdRbgFT9mzGm4IZmsrlOuVGvqXb4SKVS+pHG8/Jjk1Iujql2+Zy021y9qurW1+UYn51+6Jpd7l2Lz4+ej4Eu1YUQwr4391YW+0kiugjHi0T0o/1OiCKmUindKWvIcaOhv2UcS10rZDvlYmlCtZuekIdqYuqYqquU1zvlWmVTrlWvmmvJAkyls6pucu6xTvny+julv2pTtaP4cqfI4ZqqKuQ3OuVSQdelokqnnMTYp/5BirqXFkDqmgn8eJgfJJz/rnsBTRuxPNzYHxERPvf2Bw8f2nwe5tFcqlGX+U5i3f/UlNzf43fc3SlfePMHql0+kmvNTM2qumN3/lCn/Lb3PdEp3/+eR1S7e87c2yn/6e/+hqr70h/99055p7xBvRDBD6h9UagfPFNXrcm9rlVhPhI7p3i0/491P0n9Vhb7flfruhIzP0lET+6Wb+FqDofjlnAri32RiO6A41NEdNk2CiE8RURPERGl01HYW/Bd73WWX760edMEENdDS8TnRlxQ7bJZ6ePo/AlV1wzHO+WlhRc75Vas38qMYn1L122tfRuudQrGod8mrZaIiCFsqbrQVwjnfYtkpDI8tG95/GHHcmJ+8dVbw0imKAXE8DaPY91HL/Fztw8pV0OjU06ltEgfN+Xi+Xxe1Z28465OeXtLJLOQaPUHRfLS5LSqmz12WsYLY0yMJMItlCq0hNFsitpg796gkja+6UPXW284b8Fb2Y3/JhHdx8x3MXOWiH6GiL5yMMNyOBwHjZt+s4cQYmb+V0T050SUIqLfCiG8dGAjczgcB4pbEeMphPAnRPQnBzQWh8NxG3FLi/3msKefaA0C1XSrs7dYdPaEZchJS+t4cSL638ys3o2fPX0/9C+a1uIbL6p21bLs1MfNuqqrN0VvTPPXOuW5qY+odtdWRUcNsTW9gWnP6Gp6BxfnR+uQ/bG/Hm13dlu4U296QF08ibGP3mbKfmg1QO83+mouL/sup++6T9WloO362oqck9bPTh7Ma+m0NrnmCsVOOZuW8abjsmpXWV/qlK+tLKq6BJ6rbpMa6OL4cVcz7lmr9lls/7oXPGvf/vvdEneXdThGBL7YHY4RwdDF+I5zTx9rQ8TGAYRFDGQSsbiVVFS7pFXqlMcK2ix3BExxlXse7JRrO9o0trG80ClXK0Z8BntStS7+RIXi11WzuSkxGdXq2ssvTXLcbYGJ9q2zDnpo1rK+UihaozjeMmI8mtS6nDew/z4ebv3APcTbdEY/cifulLk6evSkqnvzje/LAZhBi+belori2Thz9A5VF4OjzqWF1zrl++7RKsP2VRHdl5cvqrpWAI9L6g38zpG5uVpl6/Pwh96iOh53+6xc/974m93hGBH4Ync4RgS+2B2OEcFQdfYQiFptt0R0jyUiigh1PGMmgsgx5W4KbphERPWG9Nlq6rqZCYl4ap0Rfa22vaav1RCTTGT8SHMQzMAN0SHLm6+pdtOzUnfP6berukpNdPbNtR1VF5QeLX20gt07QD3aBqdgH1BuWdOblBPjBouqOXNvgxLqjVZjRP0VXWTnjsyrdveclgCX5WVt8tralHuTB12/mNUBSvmimNdmjmi9vwaBU6WSuNVeef17qt3Z9Sud8uXL51Ud7n1Yc2kvWLdoPVf2HdtPTz84+Jvd4RgR+GJ3OEYEwxXjKVDcNvMYKV4JLy3jMRb3MP9wSnunVSGCqlzRJrW5SYmNLhZElF6/qsW+7c17OuVcTnvoVTbEiysNcfBXrum49PXVhU55alKbiY7OS7RcOppTdc26eIIl6/Jd0IOLyJjUjHiOpqY46W02U8J5HzMOEkiw8WyMsK6HRxcR0dT0TKd89z1v031An1vrK6oOTV7ZjIjjbAgqKCNzHGVyqmp6TMxyx+empI+KVt8urcvzUjbm2D68Ez3Rz4OOQ28RH9GtGg1yxd5qgL/ZHY4RgS92h2NEMFwPuiDeWqmUFSuRekqfhqQDuDMfmd34JBbRumm8wrIpEW9rLSEjiCvrqt3cCSE7WEk0ZVW9ut0pF0CsLJV1UMXajhxfvryg6lAsHh+f1HUT4gEYB6SUWlXtqlXZxU8MkQN6w/UT91EkzOZ08AhKi9mcfM9cTovIRSCNiJt6HOms9Hn8uKgu4+OaSuzKpYVOORh+txRYAlqgnlhOu9K4EFa0rJcfPBONsvRx/NgZ1azSlKUQ/+Wfqjq0DvGAsnU3C1wPYpIu4LZ9b9XLUokN4t3ob3aHY0Tgi93hGBH4Ync4RgRDNr2J7tjq7RS2z4k9WBSD5g+PG6J/Ly3pyKWrl9/slGfmxBQ0OT6u2y0sdMpWz80WpG11W8xtE0YPrQCvebOuI/NWrlzolJnvVHVHjsnx3LyYBK2eWABCho11zXGOXPR1GIeNwkpaqAOb/oH4EU1eY2Ml1W7+mEQSNgwN9BjQfEdwz1au6PvSqMn+QytYHn0ZcxoGmS/occwdkz2BWlXvs6yvyHynTgjp6APvfly1e/kHku4gjvVekB6UjWa7fbDbATfLI78Hf7M7HCMCX+wOx4hg6Ka3jkWpn0xiRSX4SUKvrShlzQ/CGXdhcUHVXXhDxLSpMcnmMj6tvdjis690ymMTR1Xd9NSRTvnqInCz1bVoit56y+vaG6vRELF+c9NkhAEuteKY9HEExFQiolpV+mgYNaEKYmyjIeJoNqtvda0uddaUhVadDIjx4xNTqt3cnAS1RDmtymxuikq1ACQU1jvt9An5btvb2gyawfsLXnNRQV+rgWQbLc0beOyEqEaPfvQfy3VPaxXqC/9T1LxmPzHe8scNaFFTHHRdz37vVF+6jz4XcA86h8OxB1/sDseIwBe7wzEiOATe+F2dojstc58MmGkgbQQOeTIpm1Fd2drWuuFLPxB++Pl5Mb2V17WemAbCinpNk0tMHRdX2qNgJtpeW1btsikhnpgsFVXdTk30waZJPbyzJZz1aDY7ece9qt1YUdxsq1s6eituSrq9FOi81p0yn0MCCD3f2ayY3jIw9zMzR1S7O+4ULv7Y8LWXKzJ3ZXQntt6sYAJsmei+LPSZiuRRTaf1fUe341asdfZ3vOs9nfJjH3hvp/y9b31Ltbtw4Q0ZojEL6+fRuB330NTtM6y9ZQd7x95klu2euO5Vmfm3mHmFmV+Ez2aY+WlmPtv+P92vD4fDcfgY5Cfmd4joCfPZZ4jomRDCfUT0TPvY4XC8hXFdMT6E8FfMfMZ8/DEierxd/jwRPUtEnx7kgnuiScumGu7Dic1gGgp9uOrwME60+eSV18T09sMPvKtTXnpTp3/ipojghZxOxZwCM1RuXMxQmZI2SZWvidhqDSHYR2QYPLa2VqGdiNJHTugJOQomr1plW9VtbEgf+bzMQT6rSTS2d+Q8y3GnxGT4AmlDDDELHn/lmo78SxpiAswAZ1xiUmrVIGV203gsNkG1izF3gBnHBPALhpb2wpuZlXu4cW2jU/6bZ3SKwqtXJf1TN/NEzwPdrI+cjWmau0X/G5fPu691+6Le5kMIS0RE7f9Hr9Pe4XAcMm77Bh0zP0lET97u6zgcjv642cW+zMzHQwhLzHyciFZ6NQwhPEVETxERMXMIvXbjoRx1CRxA5ICkDokVXVBU0nXr67JrvVMR8fb02x9R7a5ePtcp1w0ddQJEFzGInBMzx1W7zTUJTmklesc9l4HdeeO5Vt4SMXN8SkTVCaBAJiIqAHXy0XnNodeoyI7+hQvyXSyfXr0m40qC3gUnOJ6dEaHt1B1362ZgudhY049AHTj6kORic13Px9qaeBG2jDibAq+5DAS/lMa12rR4AYKcZrTqtQP34q+e/Qs554q2oGDQ00HvghMRcQCa8z4S9+BZXG/kvF3crBj/FSL6RLv8CSL68k3243A4hoRBTG9fIKKvE9HbmHmRmT9JRJ8joo8y81ki+mj72OFwvIUxyG78z/ao+vABj8XhcNxGHIIHXY/oHEw5ZOQNRiUKuOGjPiY6q2c1weRz8ZJEqT308GOq3cScEDKcfeXbqm59VfS8dEH0xmOntC57DVL+Vms6Ki2G1MMR6TRGKGjFELG2ckWnI5o/JmmJxwxxBtZtbsseQBIb77ScvbZgelL03llI1zQ1rX2nMIJvceEHqm4ddPhqRcxyXSYjuLVjY5qAswi6+cyURCfOHz+j2nGQOT11VPcxBrkFnv36X3fKc2fepdoVxv6mU94pb6i6oPaCBkSX/RhJV2y0Zs9OdLs+6bYGgfvGOxwjAl/sDseIYPhifA/5I/SR45GkIt1HlsHTUpaQAY5fOytkClcuPKzave+DP90p5ye0SPiN/yummxbwtVeMF9sEEGI0GtpjDD3XLLc4kkOkgciiUtGqQB3Ma8W89oyLpyVYpVAUc1XTiPGT6DXH+jEoTIq4fuykpMPKGw66N994tVNeMZx/SFLRAq+2tAmYyRVFDZmY0mazNNz34piY7yLWHn/FvMzV8Xnt3zU/L2bRO05L6qnzV7WpsIlm1i5uCd6v2AVV1SWb9zGbKV5FVdNzHH20oZ7wN7vDMSLwxe5wjAh8sTscI4Kh6+yia/RWOoIhlUf9BCOyulLfgl7erbNL2411iQx7HQgmiYg++I9+slO+5+67VN2F1451yucvLHTKSPZARFQsic5bahptKiP6dtzQHOdJIt87nZY+8ybH2uo1MQE2SlqPnpwWvffO00IusbGm+eWbcF5I9HzPHJHvOT0nOm+2qN1UK2XZS2iaPlAPTQPJSCqlTX4TsE/BJm/d+oaYwCIgLZmc3FTt5k4LqcjUrHYfnpgX4o/jp4RQ4/99/bdVuzqYSLuJVaTch5NioM+JBjeb2XFYU/ONdupvdodjROCL3eEYEQzf9NYRRbTckWC63q6oo178dL3lrVbLipVyHqY8Pn/xnGr33eee7ZSn5nU021hG+s8qHjQ9jgnwQMsbr7AN4FNv1TXHXQrMUltbkB66oHnscpBi2Yp6hYKYqN7+wLs75Zde0JxrlW2Zn9KU5s6fAdE9lZf+yjVtRmxBNGKmoCPz0mCOxAg4y3MfAWEHGxG/VhXPu3JZTHlbZT1v5aqMa+nKkqpLZ2Vcl4C/vrKlOfvrhg8QoVjd+0RrDhodZ/kAe6ZbtpouWp1NTrB+JsE9+Jvd4RgR+GJ3OEYEw9+Nb/+3dL0YqGFFkt5ivG13o6MgiiI9BW9euNQpFy5dUHWZloigOciWWjM70am0iN3jZie9BNlNNzY1DXQqJeJ5aVp2xLcMVXV+THb70xkj+jYkKOTovLSbGNeqQL0i33vOBLhk8tJnAItBw/DMMahe+Yz2jMNQElSprHpVBpE8ndbfBYlKyhDUswk8e0RE9aZYTY6cOKPqkprs3CMxyfLVK6pdE7jwuhji+m3H43jVScZSpDjoBtuP70VTvXsxm002dI/BwN/sDseIwBe7wzEi8MXucIwIhq6z76k/wbDuIXekUeeJI+SUH1Qxt+YNKMMFjApJY5MSNbazfFbV3XVSTFIhKx5oL5/T5BLlskRQpdN6ischsqs0oXXlOpi2mjXkntffGckup0wa5R3YB1jbEK+5WSC1ICLaglRTpWltYsRIOuRTrxnz1NqGXKu8oyP/6jXR9eNY9jcSk+Ip6qPPZ2E/AvuoGtNbDJ53kbnvb74upBrfgxRgW2W9/4Cw863qugLR9ie2uLmkzPYcm2qqX+PbxxvvcDj+jsEXu8MxIhi+GM/788ajmaHVx03pZim8g/Kuk88vXl5U7d58Xbysjs9qz7Ifes8HO+VV4KFf3m6qdpdffkkOTEDOFojZE1NaBE9a0nZ7Uzy8Nte1t1cJSDWOnTyl6gL8fu9AH2fueZtqhxlTT96ls8RWgb8+vS6PyNK5N1S7LRDjt7d0cEoc6znpjM/cWxTdC0VNxJGDNE+YoqqV6L4DcNCtruq5urIqHovVuoj7ubwOXqpVIRCmZXj0B3eNk1NMVV8zGnahzjF99Ml03JUKbR/4m93hGBH4Ync4RgS+2B2OEcEhuMvyXmFwKD6+mzHD2UFIH8sr2m1yZflyp5yYCK0LF8XEhiSKdo+BgWWgWtUmqQaampqavCKdF5dW1O2bhrTy0sJrnfLY2LjuA0gaN1ZE9773hx5Q7SZnhA8+bYydW+tCxri8tNApX7WkktuiD1s9F+9MKiURgtb0hiQjWeP6iySQSDiJZjgiotVVMTG+9sbrqu7yCrgkZ2RPgCN9X4Ly3+5ttu2fKrl3vjjcI2kZ9+pee1L9tPBBdHSLQdI/3cHMX2PmV5j5JWb+VPvzGWZ+mpnPtv9PX68vh8NxeBhEjI+J6JdDCPcT0SNE9IvM/A4i+gwRPRNCuI+InmkfOxyOtygGyfW2RERL7fI2M79CRCeJ6GNE9Hi72eeJ6Fki+vT1+tsTg6w4pMSXAbyB9ms3qFiPrXaA35yI6HUwL1XmdGqlb3ztS53yAw/+aKd87Lg2f72xICmEq6b/FESHdUlzkNoKv0tsROQGiM8vv/C3qu7ECfGUq8C1L57XJB0TQFBRbWqxuFwV9WJ9VUT6nW2dFqkOKaq6zEQwfhTVJ0y65RR4GKaMt2GA1NERpG/O5rSJrrwtZr9r1zTXXtyScYxPiXdktaHNd5jay3rQ9X2uFKc8mnfNPYtlruKmNT/2et4HT9Es5Cy9184NbdAx8xkiejcRPUdE8+0fgr0fhKO9z3Q4HIeNgTfomLlERH9ERL8UQtga+C3K/CQRPXlzw3M4HAeFgd7szJyh3YX+eyGEP25/vMzMx9v1x4loZb9zQwhPhRAeDiE8vF+9w+EYDq77ZufdV/hvEtErIYRfg6qvENEniOhz7f9fvu7VGDwPu30BoWhT2t5MgtrBYE1GV65chjpt8iqBmWsaTHRxXhsisjlxxSyWtGmsAH2sXb2s6kqQm216TphqEuN6ylnR+ysVHb21Dqw2GKX20gvPqXYPvvcDnXJrfEbVNYGQE8kia009Dow2s+GDOSDJTIMpctK4CBfHxSU5MX1gvjvMmWd1dmK5h81YR+bh/Y3ABGjdeRkSBXYRQqJJTV+5Z2q22OjhAcyFcdybYx/NttxFTNnrgGgQW/YgYvyjRPTPiOj7zPzd9mf/jnYX+ReZ+ZNEdIGIPj5AXw6H45AwyG78X1Pvn40PH+xwHA7H7cJQPeiYepsx8POb8Q46KKA3XEwpVbdWEdFv8Zp4YE3M6BRMs7Ni4rEkii0QM+fntAFjpyIicx7E4LGiJovMgKdZYpg7MUIOTUgXL76p2h07LmmS7rlfi9abEM2GqbKaxnMNTUY2ZVcGRPAIIw5NiqcEjo+dPK3qChNC9BGDSpLN6cd2eUkiF5OkoerSIPKjJ9+Y4bnPQAqveq0fsUXvY7wVibGrtlSdMb0pD7oBfei6AkM96s3hcLThi93hGBEcQhbXPRK63h50VtK/jZvx+4hDgnRG7/pmciL6rW+LqJef1KLpDnh0kdntL44L8UR5Q3t7zR8T0RrJFFImfWetIp5xeRM8UgGvtiYEX1j+uIU3JWBkAoJiiHQwULki40iMCN6AQJWMGSPuPqdzMsaq4X6rNUS+zWb143g0LarA3PwJuVZKq1e5jKg5SPpBREQ4DuDlt1l++3K592OUUNyGYd8ykX2G+zx0eKl+68AmV3AOOofDsQdf7A7HiMAXu8MxIjg08oqegT67jW4K/Tztetfpi6F5Zm5Wm8aKELEVQxRTaUyb3nY2gSgx0QQVqKDNzJ9RNYWS9NNalpxzcVN78jXBqy1K6Rxr6BlWq8sYIxNRtromuu1L33te1W1sy54A5kCznl+YNdi+NdDzDsspo6C2GrKvsL2j9flJ2BOowN6BzTmXBmLKSkXP99i4eCxmMzIHlnAyC6bCek2TlqhnxDy4Lc2sAmXzXMEERWa/oAn7Ol1kqwcIf7M7HCMCX+wOx4hg6GL8rWLA7LmDw1owoGxTN0VITgBi2saWJnVgIFqYPXZC1a1D8Mvs/J2qLg2yXrkuonutob3CtkHMzpugEM2PL2I3psQm0ia19Q2dOlp5yrGoNWzEz5CA16Px5ItBBEfRNJvWZrMWybW6BVjpswGmw7LhqC+MCcnIxMwRVZdAyulUSlSSTFZ/F3WvzYOliCi6CFOgCkT8yDxYmBq863tCIA9OY7fqeWsPvL/ZHY4RgS92h2NE4Ivd4RgRDF9nH0jt6O2TePN6eo9oIra6lRyvry6puq1NcW89cUryoxWLOoIK9eFtQziJEWupSOuvV0Gfb4K5LSJDXtgQ/bWLaAGVPthjiMz3RDIIy+WOiqOKRjSRbeg+2wz6vZHPghsvRKJZN9V0TkxeU9OaBCQD+v0U5N2bmdV6+eaakCQ1jZkyDX3MTAP5yJjW+xfOIee7Ma8lveuiaH/TmzUtIxHFoFZnS3x5qyQu/mZ3OEYEvtgdjhHBIZje9kQRK6IMFrSv+LpuSKbfv890WveRA/EzbcxEKPqimaxe1R5X6hfUEBXMAl97aVzz0i8uLnTKa9dANK1r0TQF37ta12Y5ZbrBKKxgSCNA3I0Nt1wauO1TMAd2vlEkt55fCVybYUaaxgRYHJNrlbd1SqYdMGluLEnqrXe8+wOqXS4v9yzL2jMuAtNhdVvUn+WrRv3BeTNaTRKj2jdYdFyXKmC+d69rKxyEaRngb3aHY0Tgi93hGBEcAnlFD9kk9N7xHOj8AwKK53a3vADBE+idVjOBE0ksXlubq5pMoQh9pHLa8y4FojZqFwFIHIiIMuhpV9OkFOjlhnMV2Z91qMtkNQEGWhcwg2ylvKPa4W58o6ZVjUAY/CJzVczZwB05b/HSa6ouDXLsPffIbvyJGU19vQoedaub2vpRrsuYk0Tu58aGHm+tJuO1qZuUOmQeP+VBh89wy3rhgYVDdzE4OcugKV57wN/sDseIwBe7wzEi8MXucIwIDsH0tn9qWUvQd6vo620EihYbDQqjnyoVraMisQWeVitrk9H2jujiZeNBx0BimTfc5QXwJosb6IWnvb1SKvrOmLwwFTHontz1uy7n5Qxp5Rhw1tfAG9DqsjGYk7r41BMZRzYr85Yyps4IzHyBtD5/4ojci1/4pKQKXI50iuzUZSH9KDd0/2Ugs1DpuVvaFJlWc2Ci+6iXbcyY7OCh6H6cMb2Zrhk81fit5Va47pudmfPM/A1mfoGZX2LmX21/PsPMTzPz2fb/6ev15XA4Dg+DiPF1IvpQCOFBInqIiJ5g5keI6DNE9EwI4T4ieqZ97HA43qIYJNdbIKI9eTbT/gtE9DEierz9+eeJ6Fki+vQA/e3+t1xk6BnX45zu2sFFGTSZsCrrqzXBmyxOrNgqdeVtEdUtn9n0jJiJTp++V9VxJOLj+de+q+pW1yXQpgVmuLExTVDR2BazEdt0Sk0ZM46/ZYg4MjmZu3xOj39ySkxbqIYEMx+Y4ihlXhtjORHd8d5WTTBNHjzeIjOOU3dK4MqpB9/VKV9+SasdDRgXp7UqgGmdMAdTxXg9YkZaS8SBsCJ4r0fzRnIfqOe7n0QPQTdsTHuDBMkMmp891c7gukJET4cQniOi+RDCUvtCS0R0tF8fDofjcDHQYg8hJCGEh4joFBG9j5kfGPQCzPwkMz/PzM/f1swuDoejL27I9BZC2KBdcf0JIlpm5uNERO3/Kz3OeSqE8HAI4eHb7PzmcDj64Lo6OzMfIaJmCGGDmQtE9BEi+s9E9BUi+gQRfa79/8u3NhR0l+03oH5VvSt7/dBYEsVUShoiNzwR0dS0kCZk86JH5/Jap0ZiyrlxrYfWrkn0Vlj/hqp77byY+lZrMo5iTuuomBJtYkzrqBXwnq2AR6jdf0A32FZLf888plvGyDajbwcIDyvl9aM0DmmVK7CPwGPaaBPSYn6M0noe3/muM53y+o5ECK5vaN74zS0xfVbL2tTZrItujkSa+bxJ2ZzaP9KPyKRfHjSLsjWv3cxek3XNhY0n67bbadxn8QxiZz9ORJ9n5hTtSgJfDCF8lZm/TkRfZOZPEtEFIvr4AH05HI5DwiC78d8jonfv8/kqEX34dgzK4XAcPIbuQbdnIuiWQtCrbUCwFsGRZ61LzOlp3rCkCyKyRUb0nZoWg8OdZ97eKecKxhQEEWA5Y066+wE5794PaM+7V39Tor5WIEJrJ9bmNaqL2W/CeKRlCnJLcXY2q/q7oDccRnwRETHw8CVgbrTeephKumTSLZdgTj7ymHi83fXDD6t2v/Wni51y0ZgY/8EHZB/4/GUxw0VZHQXYAoKKekMTccQwd6iFVKpaFcDnoFDU96wFz0ESW1VGygPyWnSZe3ud1uXz2O8C7TpLvNGvP4fD8fcUvtgdjhHBIaZ/4p5Hduccj1OQiTMy7SIIVImMJ1ULxDkkXYhSupMaeFa1slqcw+CUGLysSjm9s5vNSCBJcUITLcSRnDf3todU3b33y67463+7KhWRFk3HcvOd8mRVWzzrsHOcLoq4W2tqVYBBeEwbko4AIn4GAlXSxnIRw42xfH1333myU/75X/iRTvnMg0+odvTLMw4AAB7OSURBVGfufx0GpcXUTO6eTnllTeZmzZBoXAO+vnpdi+cRuPbl4NnJmOCfAOOPy2au0DPOvB6Vs10/Kbt31cD79GpHv0uk5/b5vXvwN7vDMSLwxe5wjAh8sTscI4LD09mtvg0fWD06kxW9sTgm+nE6o00w6A2Xy+k69BhDnvem5UwH3c0STlaB3LEOJJO1WlG1y2bFhFRpaB0qDUrf2UuTqu6nfuw9nfJzZ78l/Rve+J//+COd8vHWRVX3+1/4Tqd8ZUe+Wzpl9HKY79ikhEZzEt6XjNHLkbI+RHqPZOrIXZ3y178p5622Lqt27/2RH++Uz1/U3m8vvbjQKV+5JubG84sXVLvyJpCFlDXRRxpMgsp0aIg4GlUwP5qoN/QijJsmIu6m4j36pDe7me4GPM/f7A7HiMAXu8MxIjiELK67Akdk7GboVZTLaxG8MCbeU8WClPPFEmlg1lJdk4AZDYMeKlVtxtGZOLXIVgO5tVkRE08WUjoREc1Oy7iWlxZVXRlIHaplbdp77FHxGPvlfylzsLqhxduf+6fivbzy+rdV3cmvi1fe1nnwTitrdaVck+OaEX1Xr0r22lpN5octbyBme81qVYaLs53yckVE+vNf09e689z3O+VccUrVvbF4pVNeXBR1ZXtrXbXLZ9C8pk1qFeDQC8DnXza8gTFmxiWNGAJ5DiJMm3uYzXb7H+wCNujrwMgrHA7H3334Ync4RgS+2B2OEcFwdXbmDve6zS+WSkPaXVNXyIsOjG6wqHsTEWWAVDExppUIf9fyLaxQSMAUt7Oj9boESCCZgaDQcJBvbYmeO1bS5rVqWdxg01Wtd33nFdEvH3jHezvl9z+i9yZeeEXyx/3tX+pbmJoWF9Mp4FRcuqbzyiUQyVU3EWCXF4VgoxVjmmptekMe/YZxx90GosoWyxiRQ56I6MLCgvQ/pl2LK3W5F3ngsrf6ab0s3y1viT4gQm55Se5nudw7J4CNnDuQlAbco0x0U+a7QXV7hL/ZHY4RgS92h2NEMFQxnpk76ZXyBS2aZgzJgwJ4MAVgIAiGE41UimLDIwZiN4pllossZIFrPWjRNK6JuLu8ImahdFZ/l9kjYorL5jQhQ9KCdMvacY0WFy91yjVIW/RKaVw3BJNXHCZU1eQc/H5npfzyKy+pdkj00TJyZB3MUJjCOm/UqwqQdOyYNFeXLi10ynMnxfQWpXQf337urzrlRtDvnjyoQDPAZT85PavajRePdcpVY0ZMIEVVlJLH3UbptSC6z4Zdxo39nx2LmxGthwl/szscIwJf7A7HiGDoYnymTbucNuQSEQSIJIb7Dd2WkEyhRbpdHWiDMxmtFmCQDJJcjJW0GIwBM0miA1DQoS4LnlpssnyugQdaxgTrRGAx4Iyeg6012alfuPCmjKmqrQIzk+Jplkvr3+tGRbzLjh8V0bdl5E/kj7PehjkYVx5oskNTz0cxJ+2qda2TXLpwTvqPZA6yea3yrK2LZaFudvSLEGw0NyM03gWTbgvJNrY3tHfd9rbM3Qak7EoMr1/O7OIjtLenzT68/zldEr06PvgECuKB6uQVDsfIwxe7wzEi8MXucIwIhquzRxGYogxfewL85IlWhOJEhpkHdScx6YoxCitjuMVR18+AnotkGEREDdCpl5d1xFoMOmtuSaKwbMrmySkxBbHxOsO2Wxvaq21tTfTX9U3RPWs7JiptVXT7YkrP1dFxud61AIQdsd7fQG+4YlHPwTiknMboqlpF7x1gWirDN0IN0ImXl8Qjb/LISdVubl445fH7E+kU2etry51yznDlV3ZgHg1x+tScfJetisxjJqf3SxjMrHXLow+Xs9GayOWuLMGW4yJg+dZNdJZ7fhAM/GZvp23+DjN/tX08w8xPM/PZ9v/p6/XhcDgODzcixn+KiF6B488Q0TMhhPuI6Jn2scPheItiIDGemU8R0U8S0X8ion/T/vhjRPR4u/x52k3l/Onr97YrwsSt3uY1DIoh0kEKKAAlTW3uyYA5LzLmDewDuesyaS3uJ2Ci4i4+dalD8yByzRMRTZZEfG4YL7wCZDe1IuHF8692yqurItLOzB5V7ZKWjLlhzI8rm3LtjQXxyGObugnUmmJJC2VTk3Icg3qVJNpzrVYXT7uJKe3llwFikQoEnUxO6WCXqaN3wpi0tyGRzN34uHjT1c19z4IX5MrKJVWXAHnFDhBWWHMj3lsrZiPZhJWeWy302oT+jJqKls9e5rrbjUHf7L9ORL9CWhOZDyEsERG1/x/d70SHw/HWwHUXOzP/FBGthBC+db22Pc5/kpmfZ+bn7cabw+EYHgYR4x8lop9m5p8gojwRTTDz7xLRMjMfDyEsMfNxIlrZ7+QQwlNE9BQRUSaXfWtHCjgcf48xSH72zxLRZ4mImPlxIvq3IYSfY+b/QkSfIKLPtf9/+bpXC4HiPXKIPsveWKt6mxms/gRlS14B6h+lwdXVpiHGLlNmIFEk05UDAsum4XWvQHTcxLgmUaxCpNjyFa1fNpC/XZkV9RhP3iFRZLOzWo+ubAE5Rl4i8za2NUEF9jg9d0zVTUyIfozuw8W8JpVcvSq/71tlvW/xrrcLB355W0xec0dPqHbjEMFmXXrLQB6Cewx4H4iI1q7B91w1ue/A5LgB7sgZtqmdZe5bNpqyD5SeHjCSUD+cEXy3lt0vGFK03K041XyOiD7KzGeJ6KPtY4fD8RbFDTnVhBCepd1ddwohrBLRhw9+SA6H43ZgqB50IQQxWRnJBVMtWTGqASaeVD9vpoDeTNrkhechz1zLRN9lgUQD+eqJiJr1/bnF63XtcbV2Tby9mia10tS0eHQlDS36JiAyY0rhkjGNpcFEZckUMjkZ8+SczGn5xe+odmPA6TZuVI25+Ts65VxexrF0UaddQg/GhkmjFYN5bGJKvjOZe5sCOXjW8O83ga+9Bjz9wZgzz50Tk+XOjvZKRNWoBfOdz+hnpw5pnbpNb7291bCt9rQz7VTe596RczegQdww3Dfe4RgR+GJ3OEYEQ0//tEf00O1BJ8dsfoJiENuaII9bamNF75zRXw0pqFMgRgUzjhDkPEtsUQUPLJVZ1aaygp3jbRM8kimKmJ02/HRF4FybBa+50qT2OkNzRcv+XsP3rAD5gxVEqxXxart6TVsF7rxLdvtLE6JCNOM3VLudsojWU1NaFZidku+SLUofaUPYkQFyjMqWniv0QltZkeyvm+urqt3KFalLZQzdNUwPPhI2ey+Kz2znFCZvcBF/cFUAJXdFk3EgHNYCf7M7HCMCX+wOx4jAF7vDMSIYvs7e1nmsrtwEU5klnMzmJMoL+eDTJv0T6vopY/tQ3OjAH27TMidgXrOaLqasqqKX3LTRqeE3tGLSDDXqC3JtY2fJgdkvX5D9golpHWOUgRTFweiCEcwJkmLOz2svuUuLYkarbGoduAYc8DHotkWjD09OyBjTWU3gUa+BmTKSOYgT3W4b2q2vXlV1a1fFhIkEFVtbmuSiBfs9kXFPy8KYA3D2x3bLCMp2zwhhdW/U4fs7wvWrVJsC+/bddWzv+wBkFv5mdzhGBL7YHY4RwXDF+BAobnOTtYwcFZTIrMVbFN1zYKrJGpILlGQsAQaK7sirZoUr5D3LGs73idn5Trm6I6JppayDTBp1OW6ajKDoWjU5qYNY0PTWAA+0yKQqwoCLyPxe18HUVwae9KPzmvttHfje7jx9j6qbVOQVci/uvu/tqt2VFeHHv3BJe9c1IIVUDdIn1Yy3YQw3wAbC7GwJDx+K8cWC5sxD02SjqtWmDMtztrMNnneRyfLbJ8sqmsCs2ocPUOhR3j1WioKqw4CrBJ6/Lme60M98d30znb/ZHY4RgS92h2NE4Ivd4RgRHFrUmzWvIbmjTbecBbMORsexcVPF/F0pE9WURzJKOC823PMxRJ7lbR64puhFqHvWjHlN8cubVNQx7FVUd7T+WpoQffvMfQ90yuUtHcmVAxKJ6pbRUdGdGLS+ljFFjpeEEPId73xI1d0LuvnVVdGb54/oPYZ1yKu2DJF+RETTYI68eH6hU75yWev26D5s57tQkLkr74AL8rbm0UcOeDa88dtgIq2Di3Mc2+cPc9/puWI4tnVIfoKEKaGLgk36T5n8fE1MCd2H+FKp5VZFH+C17W92h2NE4Ivd4RgRDN2Dbs9rzIooEcglUaq3GI+kDpYjDkVym5I3jsSkgTzgdhwpSP/UMn3U0cSGYl9WT2M2wqg63Qdy3ddrmtiCgQN+ZVFSHjer2oNuAlI2V7fW9LUhtOvIrIjSluhjc13Ma13EE7GIoBjpd9GYtTY2Rb1Acx0R0XhJxHOcg2azptqlWqKSsDFrZVMyj/WqePUhxx8RUYHFHJsy8m25LNeLMSW08ThDPomEbFoxPFBV6nnB59Q+f024dqOu66zJsde1+mEQLnp/szscIwJf7A7HiGC4u/EkIlFkdodTsFueNTvYKRBNEwgeyZpMrXk4L+nKBAueSSBGZUwAB4r1qZSengJcr1aVulShpNtB8MW1VRO0AeV0VgfyYDql9VWhRzbxJ5SH3efNDR3EEmKwJoB6ceKE9qDDQKSXX35R1W3ADn8TCDDKRu04d+5sp4w750REDZj/LfA2TBc0r9/0nKgobCw0m5uiQqTh+SiVtAcdisxlk4qr1053l5fcoAlMjGgdJ/urZd0BM1g2nUT7y+vc5YW3f8DMoPA3u8MxIvDF7nCMCHyxOxwjgqGb3vZgdRpG05v1YAKvOVRbrBdeBswgZLjF60hKAXaKtOGNR/3PkhKiBxZ6zdVMCmEaFx1+AlINExE187Cv0NTjx8SXmkdft8NrWxKQFJBXqDgrQ/RRmhDz3Xde0JzyaxvioZaFOV3f1J5rl5eEqPKR9z+q6jahLaZYjo2ZCe9ttarNcpvbYm5Lwd7KRFHr7GgebCV2TqVcBw55u6dz0xhUd0baeLNfwD3YMroJJwclytgfg+ZnXyCibSJKiCgOITzMzDNE9L+I6AwRLRDRPwkhrPfqw+FwHC5uRIz/YAjhoRDCw+3jzxDRMyGE+4jomfaxw+F4i+JWxPiPEdHj7fLnaTcH3Kf7ncAkIow1PyAfm03dhJzy2YyIcK1Yi8918PxqGnGuCqQOOeBrt9xdalSGIy6fF9Mb0MBRLdFZXLe25No5ax4syLUzKU2wsbUlom8avOm4qQNmopaIu02jQrRALF5fFw+3sfEt1e7ee+/vlC9fXlR1q0BsEYH9B73FiIjuf+c7O+UZkwl2B1SNuWOSTmpzUwt/O2CWK5W0WS4N197ekvFX69bzENQfM8ZcAfgLweOyYdJyxaC+9cuq2s3/vj+hRD+e+EFhA70UcYa1y7Wr+on3g77ZAxH9BTN/i5mfbH82H0JY2r1AWCKioz3Pdjgch45B3+yPhhAuM/NRInqamX8w6AXaPw5P7pZvYoQOh+NAMNCbPYRwuf1/hYi+RETvI6JlZj5ORNT+v9Lj3KdCCA+3N/UOZtQOh+OGcd03OzOPEVEUQthul3+MiP4jEX2FiD5BRJ9r///yQFdsL3hLUJEDd8tcXuu5qEk3gZCPEhOtBXpYl9kM+MmL4N5q3XbRlNVoaF0cfxqVyc6YcSpl0S93jN5fGheChozxg8Vrb9fk2k0T5YWkmAWTzjkFUYFVGP/6ptbZp4Ck8bHHPqzqLixITrfXz0k5k9H7IEVIaV238610YvmRz6T1vT16/FSnbHO9NRKZ8AQm3/LtYyQdN/ql8cb0yvq+Z8F12fbfMyrNgJUebV9saDYb9KVnzJQ99geIiMKey23Ss8lAYvw8EX2p/VZOE9HvhxD+jJm/SURfZOZPEtEFIvr4AH05HI5DwnUXewjhHBE9uM/nq0T04e4zHA7HWxHDT/+0J8YbEbZQFNG6YHjBGUTtBMxtrViLjmhOSYxZLp1G7jr53HqgYXRc1kSlYZRdWnHaaZEQiSKSWIuEVYjKSmc051oevncL+g9GXdksSx824g657icgPVPFpI5eXBbO9zETsVYFlScBsx+zNhWuAQddOldUdds7cr3lFYngIzPfmMJ5y3jorUFEXwYiEDPGvJbPyRxYIg5tYkPzruGgAwnZEn2gCaxLOMd0TZiPoA9/XD/TnrquudogW16hD+OF+8Y7HCMCX+wOx4jAF7vDMSIYrs7O3CFqTBuySGSqYWOWK4B7q+JkL2g9MQ26c2hp/RJzp8WKDFDrOJjmOHTpTL3y0RnyTGiWzWm9PwPfs17RUV47sbiOpiI0V6lmlEMe87TWt5HHHPO0WR8HnMcNk39tBVIn4xyUK5pwcgyizXYqmiGmUhFzYQ1Mh0eOnlDtkAM/bOv+j52SHHRXgW8+mCxoOxUZfyrSk4X7KdqkZnVbhnbG5AVzZ3O99TKjded6w/KNR8rZcfQ+x3V2h2Pk4Yvd4RgRDN30tif2WA86lHMSk5pH8X0DWkaEQvLI2JjlWnU5RtILNupEPi/mr22T1omyIuKXIFXRlknPhAQYScua3jBiTX8vlNLQw6thxpgBU1NsTU3gvYeed6UpHadUvirezZgGm0iLxXVIOW25yVstGeMKmPKIiFavynEC3/PS4oJqV69JnSXPLI0LwcY4ePztbGkSzwzcz1pNqyQoC6NIH7rE8X7EEOh5ZyLReqRrsqL6wOa2PqI61t2M67m/2R2OEYEvdodjRDB0MX7POwnTOBHpVE5WQkH+uAxsTRtFQIlfVvTHPluQJsqmeIrAUysYLzzM8IpiVNp4dKGI2GxoMRszfaZNNk/kRUOxr1jUXnI55XWm0z8VgJ8tBaQfqyuXVTvcWc/ltMdiCrwNcb4xmIiIaPmK9GnVpkYD0i7BHOcNf1wD+oxN4NHqiqgCaOFotXqLsF0pweDacQw8h8brEXfq+4ngXeIzNG31yxOlvOtMFX6Au+mRbdZ7HHvH/YR7f7M7HCMCX+wOx4jAF7vDMSIYqs4ecUTFdj42S8SIHmPd+hSa4sD8YFNmQZ3tvwrphtF7z5JcZCOMZtMaUBVMWZmUjDFj2qWhrmEGmVIeXXr8adgvGBvTejri8iXRZdNZvfcxNSU6ahWi9na2dUQZ8m3UslpXzkFEYhb2FUpj2kRHLO3qDetFKPO/DXNc3tbRd6ivduULSMm8rl5d7pSnIGU1EVEDzKo23XeAfQvUeePYpmXubTbr1Y5I6874vNwQr7vK4YbX6n/tXuPoBX+zOxwjAl/sDseIYMiBMEStHuJGE8SqVFcACv4mAaeYSfGkwhyMjGxF8s45VpaGa6dMaqgoQbOZiM95YxqrIEFFWqsJTUj5lDYEHjaIYw+W1AEJCtCMuDtm6QO51mvGrFUCNWF67oiqKxYlOCWBgJliSZNt9DNXXbkihBXNNTEPWi885dVmblEE6lATOPm2tjSfXhFSOFfKmq8Pn50E5so+D4OYtfaD9mpTNaZ/KdtnLnDYt11XymY8zQ6p/TX7aQ/+Znc4RgS+2B2OEYEvdodjRDB8wsm2jtbbmbA7UgzNVcgbb0kDc2CGahr3zWxOIuJUWl+b6y30Jnxooa4MOwT5vCbRyIA7a6job4quteiWunttIIqAiLumMRPlcvI9LTknfrUt4IpPGwaMLBCCpEw6Z+TITGelXWLcVNG9d8eY1GpAXoHtbJpq1MszWT1G1G2RdLRh8ttlmjIfltCkDm67Kbiflmg0re5Fb873flCPi/WqVY+0vu8tJHvHKDq7nRT1GccAtj5/szscIwJf7A7HiGCoYnwIIkLblLl5IFBga5oALyiGXMld/N6MkXO967IgmqaNeQ3FxYypa6JICKmjIxP1lgUxPmvqlDhqOOVxzDHI4zatNIq3pQntTXZtWTzNYiQBMXNVqYqYbbnGkdBjvCQmukxWqxOoJmxtagKPMnjsKR59cy0kKkkbMX5Q7zScx8KY4eIfm5VrYbRjQ5voQhOOjYgf9TGpdZtu98Zo3TvBtGeq8Hsq86NpqMT6yNbt5ULfdzi7p/SuUoOZYuY/ZOYfMPMrzPx+Zp5h5qeZ+Wz7//T1e3I4HIeFQcX4/0pEfxZCeDvtpoJ6hYg+Q0TPhBDuI6Jn2scOh+MtikGyuE4Q0T8kon9ORBRCaBBRg5k/RkSPt5t9noieJaJP9+8tdAIQUilz6QjFbB3EwiCbIDmBFbNRTLM7uygCqV18E3yBARKJ8U7DXeUq8LRZVSALPHa5nBYXqzVQX6w4CrvMKAZGKS064vw0DOcappfCC8QN/V1qMP5gqJNb8D2L8F3qRvRdBR47G+CCHozIN2jJJZCHr1nXqh3u1CMs1TgDJ19q/JiqK5TmZEwwp82a9sJrlOW7xDua406J/F1b5GGfUn+wEc97end2kWj0HsYglHSDvNnvJqKrRPTbzPwdZv4f7dTN8yGEpd1BhCUiOtqvE4fDcbgYZLGnieg9RPTfQgjvJqIy3YDIzsxPMvPzzPx8r80Mh8Nx+zHIYl8kosUQwnPt4z+k3cW/zMzHiYja/1f2OzmE8FQI4eEQwsM2Tt3hcAwPg+Rnv8LMF5n5bSGEV2k3J/vL7b9PENHn2v+/PMgFmfdf8Al4vNVMHeo42QDkEkbvV15QrPW6BAgUUjAGu3eQAj1xe0vzxqOJBKPBLBECWtQi452WBNCVjZKHkg+qcZaQAaPlNiBtcrvXfcdrTUGos9drOiKupTzjZA5sCmsk/rCpqVV0IuyRcB8vMKujIruHii5LGbLSghiCcuPHVd3YtOjwmCqrUTMef9tioqtlL6q6xpYQayYVbWJstfbPadBXhbZm4R5V3Ccyz24QDMJLP6id/V8T0e/xboLuc0T0L2hXKvgiM3+SiC4Q0ccH7MvhcBwCBlrsIYTvEtHD+1R9+GCH43A4bheGHgjTsRl0mRuQkEEHsaCcE4A0AtM4EZmMmtaBCcR6JHgIxrzWAG5xa3pDjjT05EuMBJuFNFSNtDYjIs9cbMg3cEsDvebShmNfcamZoJBBgcFG3R6LUkbVwqaa0l6K/cTI3sEdykuuD2kEeh5SVmeuTRfGO+VcUft2FcaFmCMDZsRGQ7dL58UTMZXVwTToLVmNFlRd2JGMt9bzDsF95gc9GFHLjfoQYFhz6SB2P98xczhGBL7YHY4RgS92h2NEMOSot0CtttJmVQzlMmjMc6jLoeUmJFZHkq+TyWk9twmEi60E9PJI99GE/G5Now9HLP1nc6KLG8sYxTGM15j2dNpgy0EO7eA86zbaALdSS9Ko9jf6mWN6W3EGxqBpiPsBzUts/DCUGQpJP/I6si2dF509ZfX5vLjSZnOii6cyul1gMSta83AEuQSCmfAqmN6SMrjZdrGz4I3p7VymA+zsfLR6NBzsXvib3eEYEfhidzhGBHwQotjAF2O+SkTniWiOiK5dp/kw4OPQ8HFovBXGcaNjOB1COLJfxVAXe+eizM+HEPZz0vFx+Dh8HLdpDC7GOxwjAl/sDseI4LAW+1OHdF0LH4eGj0PjrTCOAxvDoejsDodj+HAx3uEYEQx1sTPzE8z8KjO/zsxDY6Nl5t9i5hVmfhE+GzoVNjPfwcxfa9Nxv8TMnzqMsTBznpm/wcwvtMfxq4cxDhhPqs1v+NXDGgczLzDz95n5u8z8/CGO47bRtg9tsfNulobfIKIfJ6J3ENHPMvM7hnT53yGiJ8xnh0GFHRPRL4cQ7ieiR4joF9tzMOyx1InoQyGEB4noISJ6gpkfOYRx7OFTtEtPvofDGscHQwgPganrMMZx+2jbQwhD+SOi9xPRn8PxZ4nos0O8/hkiehGOXyWi4+3ycSJ6dVhjgTF8mYg+ephjIaIiEX2biH70MMZBRKfaD/CHiOirh3VviGiBiObMZ0MdBxFNENGb1N5LO+hxDFOMP0lESO612P7ssHCoVNjMfIaI3k1Ezx3GWNqi83dplyj06bBLKHoYc/LrRPQrRITRIYcxjkBEf8HM32LmJw9pHLeVtn2Yi30/Dr6RNAUwc4mI/oiIfimEsHW99rcDIYQkhPAQ7b5Z38fMDwx7DMz8U0S0EkL41rCvvQ8eDSG8h3bVzF9k5n94CGO4Jdr262GYi32RiO6A41NEdLlH22FgICrsgwYzZ2h3of9eCOGPD3MsREQhhA3azebzxCGM41Ei+mlmXiCiPyCiDzHz7x7COCiEcLn9f4WIvkRE7zuEcdwSbfv1MMzF/k0iuo+Z72qz1P4MEX1liNe3+ArtUmAT3QAV9q2Ad0nVfpOIXgkh/NphjYWZjzDzVLtcIKKPENEPhj2OEMJnQwinQghnaPd5+D8hhJ8b9jiYeYyZx/fKRPRjRPTisMcRQrhCRBeZ+W3tj/Zo2w9mHLd748NsNPwEEb1GRG8Q0b8f4nW/QERLRNSk3V/PTxLRLO1uDJ1t/58Zwjgeo13V5XtE9N32308MeyxE9MNE9J32OF4kov/Q/nzocwJjepxkg27Y83E3Eb3Q/ntp79k8pGfkISJ6vn1v/jcRTR/UONyDzuEYEbgHncMxIvDF7nCMCHyxOxwjAl/sDseIwBe7wzEi8MXucIwIfLE7HCMCX+wOx4jg/wPRGGUPMPTUPwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 25\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1\n",
    "Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n",
    "    - num_px (= height = width of a training image)\n",
    "Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921fe679a632ec7ec9963069fa405725",
     "grade": false,
     "grade_id": "cell-c4e7e9c1f174eb83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "#(≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig[0].shape[0]\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output for m_train, m_test and num_px**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td> m_train </td>\n",
    "    <td> 209 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>m_test</td>\n",
    "    <td> 50 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>num_px</td>\n",
    "    <td> 64 </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2\n",
    "Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: \n",
    "```python\n",
    "X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a2aa62bdd8c01450111b758ef159aec",
     "grade": false,
     "grade_id": "cell-0f43921062c34e50",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "#(≈ 2 lines of code)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "# Check that the first 10 pixels of the second image are in the correct place\n",
    "assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td>train_set_x_flatten shape</td>\n",
    "    <td> (12288, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>train_set_y shape</td>\n",
    "    <td>(1, 209)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_x_flatten shape</td>\n",
    "    <td>(12288, 50)</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>test_set_y shape</td>\n",
    "    <td>(1, 50)</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\n",
    "\n",
    "<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> \n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "    \n",
    "**What you need to remember:**\n",
    "\n",
    "Common steps for pre-processing a new dataset are:\n",
    "- Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)\n",
    "- Reshape the datasets such that each example is now a vector of size (num_px \\* num_px \\* 3, 1)\n",
    "- \"Standardize\" the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
    "\n",
    "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
    "\n",
    "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Helper functions\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - sigmoid\n",
    "Using your code from \"Python Basics\", implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "239ab1cf1028b721fd14f31b8103c40d",
     "grade": false,
     "grade_id": "cell-520521c430352f3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    #(≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0483e6820669111a9c5914d8b24bc315",
     "grade": true,
     "grade_id": "cell-30ea3151cab9c491",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62245933 0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.5, 0, 2.0])\n",
    "output = sigmoid(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-2'></a>\n",
    "### 4.2 - Initializing parameters\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - initialize_with_zeros\n",
    "Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4a37e375a85ddab7274a33abf46bb7c",
     "grade": false,
     "grade_id": "cell-befa9335e479864e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias) of type float\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 2 lines of code)\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0.0\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4c13b0eafa46ca94de21b41faea8c58",
     "grade": true,
     "grade_id": "cell-a3b6699f145f3a3f",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.]\n",
      " [0.]]\n",
      "b = 0.0\n",
      "\u001b[92mFirst test passed!\n",
      "\u001b[92mSecond test passed!\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "\n",
    "assert type(b) == float\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))\n",
    "\n",
    "initialize_with_zeros_test_1(initialize_with_zeros)\n",
    "initialize_with_zeros_test_2(initialize_with_zeros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-3'></a>\n",
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - propagate\n",
    "Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff9081e51809aef5e93bc1c21dc9b7b",
     "grade": false,
     "grade_id": "cell-11af17e28077b3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    #(≈ 2 lines of code)\n",
    "    # compute activation\n",
    "    # A = ...\n",
    "    # compute cost by using np.dot to perform multiplication. \n",
    "    # And don't use loops for the sum.\n",
    "    # cost = ...                                \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    #A = sigmoid(np.dot(w.T,X)+b)\n",
    "    A = sigmoid(np.dot(w.T,X)+b) \n",
    "    #cost = -(1/m) * np.sum((np.dot(Y,np.log(A)) + np.dot((1 - Y),np.log(1-A)))\n",
    "    #cost = (-1/m) * np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))\n",
    "    cost = (-1 / m) * np.sum( Y * np.log(A) + (1-Y) * np.log(1-A) )   \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    #(≈ 2 lines of code)\n",
    "    # dw = ...\n",
    "    # db = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dw = (1/m) * np.dot(X, (A - Y).T)\n",
    "    db = (1/m) * np.sum(A-Y)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8a1a4b1ff8d70ac609d721490b4d826",
     "grade": true,
     "grade_id": "cell-d1594d75b61dd554",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[ 0.25071532]\n",
      " [-0.06604096]]\n",
      "db = -0.1250040450043965\n",
      "cost = 0.15900537707692405\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "assert type(grads[\"dw\"]) == np.ndarray\n",
    "assert grads[\"dw\"].shape == (2, 1)\n",
    "assert type(grads[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))\n",
    "\n",
    "propagate_test(propagate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "```\n",
    "dw = [[ 0.25071532]\n",
    " [-0.06604096]]\n",
    "db = -0.1250040450043965\n",
    "cost = 0.15900537707692405\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-4'></a>\n",
    "### 4.4 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 6 - optimize\n",
    "Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d9b4c1a780bf141c8eb48e06cbb494",
     "grade": false,
     "grade_id": "cell-616d6883e807448d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    #w = copy.deepcopy(w)\n",
    "    #b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # (≈ 1 lines of code)\n",
    "        # Cost and gradient calculation \n",
    "        # grads, cost = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        w = w - (learning_rate*dw)\n",
    "        b = b - learning_rate*db\n",
    "        # YOUR CODE STARTS HERE\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b65a5c90f86a990614156e41f64b4678",
     "grade": true,
     "grade_id": "cell-8e3d43fbb82a8901",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.80956046]\n",
      " [2.0508202 ]]\n",
      "b = 1.5948713189708588\n",
      "dw = [[ 0.17860505]\n",
      " [-0.04840656]]\n",
      "db = -0.08888460336847771\n",
      "Costs = [array(0.15900538)]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))\n",
    "\n",
    "optimize_test(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.15900538)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-7'></a>\n",
    "### Exercise 7 - predict\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56419b97ebf382a8f93ac2873988887",
     "grade": false,
     "grade_id": "cell-d6f924f49c51dc2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    #(≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T,X)+b)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        #(≈ 4 lines of code)\n",
    "        # if A[0, i] > ____ :\n",
    "        #     Y_prediction[0,i] = \n",
    "        # else:\n",
    "        #     Y_prediction[0,i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0,i] = 1.0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0.0\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3ea12608f15798d542a07c1bc9f561b",
     "grade": true,
     "grade_id": "cell-90b1fb967269548c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))\n",
    "\n",
    "predict_test(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What to remember:**\n",
    "    \n",
    "You've implemented several functions that:\n",
    "- Initialize (w,b)\n",
    "- Optimize the loss iteratively to learn parameters (w,b):\n",
    "    - Computing the cost and its gradient \n",
    "    - Updating the parameters using gradient descent\n",
    "- Use the learned (w,b) to predict the labels for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercise 8 - model\n",
    "Implement the model function. Use the following notation:\n",
    "    - Y_prediction_test for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - parameters, grads, costs for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f23cca6cfb750397e5d2ac44977e2c2a",
     "grade": false,
     "grade_id": "cell-6dcba5967c4cbf8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to True to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    # (≈ 1 line of code)   \n",
    "    # initialize parameters with zeros \n",
    "    \n",
    "    dim = X_train.shape[0]\n",
    "    w, b = initialize_with_zeros(dim)\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Gradient descent \n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations=num_iterations, learning_rate=learning_rate, print_cost=print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"params\"\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b468bc5ddf6ecc5c7dbcb9a02cfe0216",
     "grade": true,
     "grade_id": "cell-4170e070f3cde17e",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "from public_tests import *\n",
    "\n",
    "model_test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass all the tests, run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 1: 0.741029\n",
      "Cost after iteration 2: 0.753154\n",
      "Cost after iteration 3: 0.866709\n",
      "Cost after iteration 4: 0.768564\n",
      "Cost after iteration 5: 0.897014\n",
      "Cost after iteration 6: 0.755613\n",
      "Cost after iteration 7: 0.880437\n",
      "Cost after iteration 8: 0.751485\n",
      "Cost after iteration 9: 0.877133\n",
      "Cost after iteration 10: 0.744940\n",
      "Cost after iteration 11: 0.869479\n",
      "Cost after iteration 12: 0.739538\n",
      "Cost after iteration 13: 0.863181\n",
      "Cost after iteration 14: 0.734114\n",
      "Cost after iteration 15: 0.856499\n",
      "Cost after iteration 16: 0.728988\n",
      "Cost after iteration 17: 0.849995\n",
      "Cost after iteration 18: 0.724030\n",
      "Cost after iteration 19: 0.843519\n",
      "Cost after iteration 20: 0.719249\n",
      "Cost after iteration 21: 0.837130\n",
      "Cost after iteration 22: 0.714618\n",
      "Cost after iteration 23: 0.830820\n",
      "Cost after iteration 24: 0.710124\n",
      "Cost after iteration 25: 0.824597\n",
      "Cost after iteration 26: 0.705753\n",
      "Cost after iteration 27: 0.818463\n",
      "Cost after iteration 28: 0.701493\n",
      "Cost after iteration 29: 0.812419\n",
      "Cost after iteration 30: 0.697336\n",
      "Cost after iteration 31: 0.806465\n",
      "Cost after iteration 32: 0.693272\n",
      "Cost after iteration 33: 0.800601\n",
      "Cost after iteration 34: 0.689294\n",
      "Cost after iteration 35: 0.794826\n",
      "Cost after iteration 36: 0.685395\n",
      "Cost after iteration 37: 0.789137\n",
      "Cost after iteration 38: 0.681570\n",
      "Cost after iteration 39: 0.783533\n",
      "Cost after iteration 40: 0.677814\n",
      "Cost after iteration 41: 0.778011\n",
      "Cost after iteration 42: 0.674123\n",
      "Cost after iteration 43: 0.772570\n",
      "Cost after iteration 44: 0.670491\n",
      "Cost after iteration 45: 0.767208\n",
      "Cost after iteration 46: 0.666917\n",
      "Cost after iteration 47: 0.761921\n",
      "Cost after iteration 48: 0.663395\n",
      "Cost after iteration 49: 0.756707\n",
      "Cost after iteration 50: 0.659925\n",
      "Cost after iteration 51: 0.751565\n",
      "Cost after iteration 52: 0.656503\n",
      "Cost after iteration 53: 0.746492\n",
      "Cost after iteration 54: 0.653126\n",
      "Cost after iteration 55: 0.741486\n",
      "Cost after iteration 56: 0.649792\n",
      "Cost after iteration 57: 0.736545\n",
      "Cost after iteration 58: 0.646500\n",
      "Cost after iteration 59: 0.731667\n",
      "Cost after iteration 60: 0.643248\n",
      "Cost after iteration 61: 0.726850\n",
      "Cost after iteration 62: 0.640033\n",
      "Cost after iteration 63: 0.722092\n",
      "Cost after iteration 64: 0.636854\n",
      "Cost after iteration 65: 0.717391\n",
      "Cost after iteration 66: 0.633710\n",
      "Cost after iteration 67: 0.712746\n",
      "Cost after iteration 68: 0.630599\n",
      "Cost after iteration 69: 0.708155\n",
      "Cost after iteration 70: 0.627521\n",
      "Cost after iteration 71: 0.703616\n",
      "Cost after iteration 72: 0.624473\n",
      "Cost after iteration 73: 0.699129\n",
      "Cost after iteration 74: 0.621455\n",
      "Cost after iteration 75: 0.694692\n",
      "Cost after iteration 76: 0.618465\n",
      "Cost after iteration 77: 0.690302\n",
      "Cost after iteration 78: 0.615503\n",
      "Cost after iteration 79: 0.685961\n",
      "Cost after iteration 80: 0.612568\n",
      "Cost after iteration 81: 0.681665\n",
      "Cost after iteration 82: 0.609658\n",
      "Cost after iteration 83: 0.677413\n",
      "Cost after iteration 84: 0.606773\n",
      "Cost after iteration 85: 0.673206\n",
      "Cost after iteration 86: 0.603913\n",
      "Cost after iteration 87: 0.669041\n",
      "Cost after iteration 88: 0.601076\n",
      "Cost after iteration 89: 0.664918\n",
      "Cost after iteration 90: 0.598262\n",
      "Cost after iteration 91: 0.660836\n",
      "Cost after iteration 92: 0.595469\n",
      "Cost after iteration 93: 0.656794\n",
      "Cost after iteration 94: 0.592699\n",
      "Cost after iteration 95: 0.652791\n",
      "Cost after iteration 96: 0.589948\n",
      "Cost after iteration 97: 0.648826\n",
      "Cost after iteration 98: 0.587219\n",
      "Cost after iteration 99: 0.644898\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 101: 0.641007\n",
      "Cost after iteration 102: 0.581817\n",
      "Cost after iteration 103: 0.637152\n",
      "Cost after iteration 104: 0.579145\n",
      "Cost after iteration 105: 0.633332\n",
      "Cost after iteration 106: 0.576491\n",
      "Cost after iteration 107: 0.629546\n",
      "Cost after iteration 108: 0.573854\n",
      "Cost after iteration 109: 0.625795\n",
      "Cost after iteration 110: 0.571235\n",
      "Cost after iteration 111: 0.622076\n",
      "Cost after iteration 112: 0.568633\n",
      "Cost after iteration 113: 0.618391\n",
      "Cost after iteration 114: 0.566047\n",
      "Cost after iteration 115: 0.614737\n",
      "Cost after iteration 116: 0.563478\n",
      "Cost after iteration 117: 0.611115\n",
      "Cost after iteration 118: 0.560924\n",
      "Cost after iteration 119: 0.607524\n",
      "Cost after iteration 120: 0.558386\n",
      "Cost after iteration 121: 0.603964\n",
      "Cost after iteration 122: 0.555863\n",
      "Cost after iteration 123: 0.600433\n",
      "Cost after iteration 124: 0.553354\n",
      "Cost after iteration 125: 0.596932\n",
      "Cost after iteration 126: 0.550860\n",
      "Cost after iteration 127: 0.593460\n",
      "Cost after iteration 128: 0.548381\n",
      "Cost after iteration 129: 0.590017\n",
      "Cost after iteration 130: 0.545915\n",
      "Cost after iteration 131: 0.586602\n",
      "Cost after iteration 132: 0.543463\n",
      "Cost after iteration 133: 0.583214\n",
      "Cost after iteration 134: 0.541025\n",
      "Cost after iteration 135: 0.579854\n",
      "Cost after iteration 136: 0.538600\n",
      "Cost after iteration 137: 0.576521\n",
      "Cost after iteration 138: 0.536188\n",
      "Cost after iteration 139: 0.573214\n",
      "Cost after iteration 140: 0.533788\n",
      "Cost after iteration 141: 0.569934\n",
      "Cost after iteration 142: 0.531401\n",
      "Cost after iteration 143: 0.566680\n",
      "Cost after iteration 144: 0.529027\n",
      "Cost after iteration 145: 0.563451\n",
      "Cost after iteration 146: 0.526665\n",
      "Cost after iteration 147: 0.560247\n",
      "Cost after iteration 148: 0.524314\n",
      "Cost after iteration 149: 0.557068\n",
      "Cost after iteration 150: 0.521976\n",
      "Cost after iteration 151: 0.553913\n",
      "Cost after iteration 152: 0.519649\n",
      "Cost after iteration 153: 0.550783\n",
      "Cost after iteration 154: 0.517333\n",
      "Cost after iteration 155: 0.547677\n",
      "Cost after iteration 156: 0.515029\n",
      "Cost after iteration 157: 0.544594\n",
      "Cost after iteration 158: 0.512736\n",
      "Cost after iteration 159: 0.541535\n",
      "Cost after iteration 160: 0.510454\n",
      "Cost after iteration 161: 0.538499\n",
      "Cost after iteration 162: 0.508183\n",
      "Cost after iteration 163: 0.535485\n",
      "Cost after iteration 164: 0.505922\n",
      "Cost after iteration 165: 0.532494\n",
      "Cost after iteration 166: 0.503673\n",
      "Cost after iteration 167: 0.529526\n",
      "Cost after iteration 168: 0.501433\n",
      "Cost after iteration 169: 0.526579\n",
      "Cost after iteration 170: 0.499204\n",
      "Cost after iteration 171: 0.523655\n",
      "Cost after iteration 172: 0.496986\n",
      "Cost after iteration 173: 0.520752\n",
      "Cost after iteration 174: 0.494777\n",
      "Cost after iteration 175: 0.517871\n",
      "Cost after iteration 176: 0.492578\n",
      "Cost after iteration 177: 0.515010\n",
      "Cost after iteration 178: 0.490390\n",
      "Cost after iteration 179: 0.512171\n",
      "Cost after iteration 180: 0.488211\n",
      "Cost after iteration 181: 0.509352\n",
      "Cost after iteration 182: 0.486042\n",
      "Cost after iteration 183: 0.506555\n",
      "Cost after iteration 184: 0.483883\n",
      "Cost after iteration 185: 0.503777\n",
      "Cost after iteration 186: 0.481733\n",
      "Cost after iteration 187: 0.501020\n",
      "Cost after iteration 188: 0.479593\n",
      "Cost after iteration 189: 0.498283\n",
      "Cost after iteration 190: 0.477462\n",
      "Cost after iteration 191: 0.495566\n",
      "Cost after iteration 192: 0.475341\n",
      "Cost after iteration 193: 0.492868\n",
      "Cost after iteration 194: 0.473229\n",
      "Cost after iteration 195: 0.490191\n",
      "Cost after iteration 196: 0.471127\n",
      "Cost after iteration 197: 0.487532\n",
      "Cost after iteration 198: 0.469033\n",
      "Cost after iteration 199: 0.484894\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 201: 0.482274\n",
      "Cost after iteration 202: 0.464874\n",
      "Cost after iteration 203: 0.479674\n",
      "Cost after iteration 204: 0.462808\n",
      "Cost after iteration 205: 0.477092\n",
      "Cost after iteration 206: 0.460751\n",
      "Cost after iteration 207: 0.474529\n",
      "Cost after iteration 208: 0.458704\n",
      "Cost after iteration 209: 0.471986\n",
      "Cost after iteration 210: 0.456665\n",
      "Cost after iteration 211: 0.469461\n",
      "Cost after iteration 212: 0.454635\n",
      "Cost after iteration 213: 0.466954\n",
      "Cost after iteration 214: 0.452615\n",
      "Cost after iteration 215: 0.464466\n",
      "Cost after iteration 216: 0.450603\n",
      "Cost after iteration 217: 0.461997\n",
      "Cost after iteration 218: 0.448601\n",
      "Cost after iteration 219: 0.459546\n",
      "Cost after iteration 220: 0.446607\n",
      "Cost after iteration 221: 0.457114\n",
      "Cost after iteration 222: 0.444623\n",
      "Cost after iteration 223: 0.454700\n",
      "Cost after iteration 224: 0.442648\n",
      "Cost after iteration 225: 0.452304\n",
      "Cost after iteration 226: 0.440682\n",
      "Cost after iteration 227: 0.449927\n",
      "Cost after iteration 228: 0.438725\n",
      "Cost after iteration 229: 0.447568\n",
      "Cost after iteration 230: 0.436777\n",
      "Cost after iteration 231: 0.445227\n",
      "Cost after iteration 232: 0.434839\n",
      "Cost after iteration 233: 0.442905\n",
      "Cost after iteration 234: 0.432910\n",
      "Cost after iteration 235: 0.440602\n",
      "Cost after iteration 236: 0.430991\n",
      "Cost after iteration 237: 0.438316\n",
      "Cost after iteration 238: 0.429081\n",
      "Cost after iteration 239: 0.436049\n",
      "Cost after iteration 240: 0.427181\n",
      "Cost after iteration 241: 0.433801\n",
      "Cost after iteration 242: 0.425291\n",
      "Cost after iteration 243: 0.431572\n",
      "Cost after iteration 244: 0.423411\n",
      "Cost after iteration 245: 0.429361\n",
      "Cost after iteration 246: 0.421541\n",
      "Cost after iteration 247: 0.427170\n",
      "Cost after iteration 248: 0.419681\n",
      "Cost after iteration 249: 0.424997\n",
      "Cost after iteration 250: 0.417832\n",
      "Cost after iteration 251: 0.422844\n",
      "Cost after iteration 252: 0.415993\n",
      "Cost after iteration 253: 0.420710\n",
      "Cost after iteration 254: 0.414165\n",
      "Cost after iteration 255: 0.418595\n",
      "Cost after iteration 256: 0.412349\n",
      "Cost after iteration 257: 0.416501\n",
      "Cost after iteration 258: 0.410544\n",
      "Cost after iteration 259: 0.414426\n",
      "Cost after iteration 260: 0.408750\n",
      "Cost after iteration 261: 0.412372\n",
      "Cost after iteration 262: 0.406969\n",
      "Cost after iteration 263: 0.410339\n",
      "Cost after iteration 264: 0.405199\n",
      "Cost after iteration 265: 0.408327\n",
      "Cost after iteration 266: 0.403443\n",
      "Cost after iteration 267: 0.406336\n",
      "Cost after iteration 268: 0.401699\n",
      "Cost after iteration 269: 0.404366\n",
      "Cost after iteration 270: 0.399969\n",
      "Cost after iteration 271: 0.402419\n",
      "Cost after iteration 272: 0.398253\n",
      "Cost after iteration 273: 0.400494\n",
      "Cost after iteration 274: 0.396552\n",
      "Cost after iteration 275: 0.398592\n",
      "Cost after iteration 276: 0.394865\n",
      "Cost after iteration 277: 0.396713\n",
      "Cost after iteration 278: 0.393193\n",
      "Cost after iteration 279: 0.394859\n",
      "Cost after iteration 280: 0.391538\n",
      "Cost after iteration 281: 0.393029\n",
      "Cost after iteration 282: 0.389899\n",
      "Cost after iteration 283: 0.391224\n",
      "Cost after iteration 284: 0.388277\n",
      "Cost after iteration 285: 0.389445\n",
      "Cost after iteration 286: 0.386673\n",
      "Cost after iteration 287: 0.387691\n",
      "Cost after iteration 288: 0.385088\n",
      "Cost after iteration 289: 0.385965\n",
      "Cost after iteration 290: 0.383522\n",
      "Cost after iteration 291: 0.384267\n",
      "Cost after iteration 292: 0.381976\n",
      "Cost after iteration 293: 0.382596\n",
      "Cost after iteration 294: 0.380450\n",
      "Cost after iteration 295: 0.380955\n",
      "Cost after iteration 296: 0.378946\n",
      "Cost after iteration 297: 0.379343\n",
      "Cost after iteration 298: 0.377465\n",
      "Cost after iteration 299: 0.377761\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 301: 0.376211\n",
      "Cost after iteration 302: 0.374573\n",
      "Cost after iteration 303: 0.374692\n",
      "Cost after iteration 304: 0.373163\n",
      "Cost after iteration 305: 0.373205\n",
      "Cost after iteration 306: 0.371779\n",
      "Cost after iteration 307: 0.371751\n",
      "Cost after iteration 308: 0.370422\n",
      "Cost after iteration 309: 0.370331\n",
      "Cost after iteration 310: 0.369092\n",
      "Cost after iteration 311: 0.368944\n",
      "Cost after iteration 312: 0.367789\n",
      "Cost after iteration 313: 0.367592\n",
      "Cost after iteration 314: 0.366515\n",
      "Cost after iteration 315: 0.366274\n",
      "Cost after iteration 316: 0.365270\n",
      "Cost after iteration 317: 0.364991\n",
      "Cost after iteration 318: 0.364054\n",
      "Cost after iteration 319: 0.363743\n",
      "Cost after iteration 320: 0.362867\n",
      "Cost after iteration 321: 0.362530\n",
      "Cost after iteration 322: 0.361711\n",
      "Cost after iteration 323: 0.361352\n",
      "Cost after iteration 324: 0.360584\n",
      "Cost after iteration 325: 0.360207\n",
      "Cost after iteration 326: 0.359487\n",
      "Cost after iteration 327: 0.359097\n",
      "Cost after iteration 328: 0.358420\n",
      "Cost after iteration 329: 0.358020\n",
      "Cost after iteration 330: 0.357382\n",
      "Cost after iteration 331: 0.356975\n",
      "Cost after iteration 332: 0.356373\n",
      "Cost after iteration 333: 0.355962\n",
      "Cost after iteration 334: 0.355392\n",
      "Cost after iteration 335: 0.354979\n",
      "Cost after iteration 336: 0.354438\n",
      "Cost after iteration 337: 0.354025\n",
      "Cost after iteration 338: 0.353511\n",
      "Cost after iteration 339: 0.353099\n",
      "Cost after iteration 340: 0.352608\n",
      "Cost after iteration 341: 0.352200\n",
      "Cost after iteration 342: 0.351730\n",
      "Cost after iteration 343: 0.351326\n",
      "Cost after iteration 344: 0.350874\n",
      "Cost after iteration 345: 0.350474\n",
      "Cost after iteration 346: 0.350039\n",
      "Cost after iteration 347: 0.349645\n",
      "Cost after iteration 348: 0.349224\n",
      "Cost after iteration 349: 0.348835\n",
      "Cost after iteration 350: 0.348427\n",
      "Cost after iteration 351: 0.348045\n",
      "Cost after iteration 352: 0.347647\n",
      "Cost after iteration 353: 0.347270\n",
      "Cost after iteration 354: 0.346883\n",
      "Cost after iteration 355: 0.346512\n",
      "Cost after iteration 356: 0.346133\n",
      "Cost after iteration 357: 0.345767\n",
      "Cost after iteration 358: 0.345396\n",
      "Cost after iteration 359: 0.345035\n",
      "Cost after iteration 360: 0.344671\n",
      "Cost after iteration 361: 0.344314\n",
      "Cost after iteration 362: 0.343956\n",
      "Cost after iteration 363: 0.343603\n",
      "Cost after iteration 364: 0.343250\n",
      "Cost after iteration 365: 0.342901\n",
      "Cost after iteration 366: 0.342553\n",
      "Cost after iteration 367: 0.342208\n",
      "Cost after iteration 368: 0.341863\n",
      "Cost after iteration 369: 0.341521\n",
      "Cost after iteration 370: 0.341180\n",
      "Cost after iteration 371: 0.340841\n",
      "Cost after iteration 372: 0.340503\n",
      "Cost after iteration 373: 0.340167\n",
      "Cost after iteration 374: 0.339832\n",
      "Cost after iteration 375: 0.339499\n",
      "Cost after iteration 376: 0.339166\n",
      "Cost after iteration 377: 0.338835\n",
      "Cost after iteration 378: 0.338505\n",
      "Cost after iteration 379: 0.338176\n",
      "Cost after iteration 380: 0.337847\n",
      "Cost after iteration 381: 0.337520\n",
      "Cost after iteration 382: 0.337194\n",
      "Cost after iteration 383: 0.336869\n",
      "Cost after iteration 384: 0.336545\n",
      "Cost after iteration 385: 0.336221\n",
      "Cost after iteration 386: 0.335899\n",
      "Cost after iteration 387: 0.335577\n",
      "Cost after iteration 388: 0.335256\n",
      "Cost after iteration 389: 0.334936\n",
      "Cost after iteration 390: 0.334616\n",
      "Cost after iteration 391: 0.334298\n",
      "Cost after iteration 392: 0.333980\n",
      "Cost after iteration 393: 0.333663\n",
      "Cost after iteration 394: 0.333346\n",
      "Cost after iteration 395: 0.333031\n",
      "Cost after iteration 396: 0.332716\n",
      "Cost after iteration 397: 0.332402\n",
      "Cost after iteration 398: 0.332088\n",
      "Cost after iteration 399: 0.331775\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 401: 0.331152\n",
      "Cost after iteration 402: 0.330841\n",
      "Cost after iteration 403: 0.330531\n",
      "Cost after iteration 404: 0.330222\n",
      "Cost after iteration 405: 0.329913\n",
      "Cost after iteration 406: 0.329605\n",
      "Cost after iteration 407: 0.329297\n",
      "Cost after iteration 408: 0.328991\n",
      "Cost after iteration 409: 0.328685\n",
      "Cost after iteration 410: 0.328379\n",
      "Cost after iteration 411: 0.328074\n",
      "Cost after iteration 412: 0.327770\n",
      "Cost after iteration 413: 0.327467\n",
      "Cost after iteration 414: 0.327164\n",
      "Cost after iteration 415: 0.326862\n",
      "Cost after iteration 416: 0.326560\n",
      "Cost after iteration 417: 0.326259\n",
      "Cost after iteration 418: 0.325959\n",
      "Cost after iteration 419: 0.325659\n",
      "Cost after iteration 420: 0.325360\n",
      "Cost after iteration 421: 0.325061\n",
      "Cost after iteration 422: 0.324764\n",
      "Cost after iteration 423: 0.324466\n",
      "Cost after iteration 424: 0.324170\n",
      "Cost after iteration 425: 0.323874\n",
      "Cost after iteration 426: 0.323578\n",
      "Cost after iteration 427: 0.323284\n",
      "Cost after iteration 428: 0.322989\n",
      "Cost after iteration 429: 0.322696\n",
      "Cost after iteration 430: 0.322403\n",
      "Cost after iteration 431: 0.322111\n",
      "Cost after iteration 432: 0.321819\n",
      "Cost after iteration 433: 0.321528\n",
      "Cost after iteration 434: 0.321237\n",
      "Cost after iteration 435: 0.320947\n",
      "Cost after iteration 436: 0.320658\n",
      "Cost after iteration 437: 0.320369\n",
      "Cost after iteration 438: 0.320081\n",
      "Cost after iteration 439: 0.319793\n",
      "Cost after iteration 440: 0.319506\n",
      "Cost after iteration 441: 0.319219\n",
      "Cost after iteration 442: 0.318933\n",
      "Cost after iteration 443: 0.318648\n",
      "Cost after iteration 444: 0.318363\n",
      "Cost after iteration 445: 0.318079\n",
      "Cost after iteration 446: 0.317796\n",
      "Cost after iteration 447: 0.317513\n",
      "Cost after iteration 448: 0.317230\n",
      "Cost after iteration 449: 0.316948\n",
      "Cost after iteration 450: 0.316667\n",
      "Cost after iteration 451: 0.316386\n",
      "Cost after iteration 452: 0.316106\n",
      "Cost after iteration 453: 0.315826\n",
      "Cost after iteration 454: 0.315547\n",
      "Cost after iteration 455: 0.315268\n",
      "Cost after iteration 456: 0.314990\n",
      "Cost after iteration 457: 0.314713\n",
      "Cost after iteration 458: 0.314436\n",
      "Cost after iteration 459: 0.314159\n",
      "Cost after iteration 460: 0.313883\n",
      "Cost after iteration 461: 0.313608\n",
      "Cost after iteration 462: 0.313333\n",
      "Cost after iteration 463: 0.313059\n",
      "Cost after iteration 464: 0.312785\n",
      "Cost after iteration 465: 0.312512\n",
      "Cost after iteration 466: 0.312240\n",
      "Cost after iteration 467: 0.311967\n",
      "Cost after iteration 468: 0.311696\n",
      "Cost after iteration 469: 0.311425\n",
      "Cost after iteration 470: 0.311154\n",
      "Cost after iteration 471: 0.310884\n",
      "Cost after iteration 472: 0.310615\n",
      "Cost after iteration 473: 0.310346\n",
      "Cost after iteration 474: 0.310077\n",
      "Cost after iteration 475: 0.309809\n",
      "Cost after iteration 476: 0.309542\n",
      "Cost after iteration 477: 0.309275\n",
      "Cost after iteration 478: 0.309008\n",
      "Cost after iteration 479: 0.308743\n",
      "Cost after iteration 480: 0.308477\n",
      "Cost after iteration 481: 0.308212\n",
      "Cost after iteration 482: 0.307948\n",
      "Cost after iteration 483: 0.307684\n",
      "Cost after iteration 484: 0.307421\n",
      "Cost after iteration 485: 0.307158\n",
      "Cost after iteration 486: 0.306895\n",
      "Cost after iteration 487: 0.306633\n",
      "Cost after iteration 488: 0.306372\n",
      "Cost after iteration 489: 0.306111\n",
      "Cost after iteration 490: 0.305851\n",
      "Cost after iteration 491: 0.305591\n",
      "Cost after iteration 492: 0.305331\n",
      "Cost after iteration 493: 0.305072\n",
      "Cost after iteration 494: 0.304814\n",
      "Cost after iteration 495: 0.304556\n",
      "Cost after iteration 496: 0.304298\n",
      "Cost after iteration 497: 0.304041\n",
      "Cost after iteration 498: 0.303785\n",
      "Cost after iteration 499: 0.303529\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 501: 0.303018\n",
      "Cost after iteration 502: 0.302763\n",
      "Cost after iteration 503: 0.302509\n",
      "Cost after iteration 504: 0.302255\n",
      "Cost after iteration 505: 0.302002\n",
      "Cost after iteration 506: 0.301749\n",
      "Cost after iteration 507: 0.301497\n",
      "Cost after iteration 508: 0.301245\n",
      "Cost after iteration 509: 0.300994\n",
      "Cost after iteration 510: 0.300743\n",
      "Cost after iteration 511: 0.300492\n",
      "Cost after iteration 512: 0.300242\n",
      "Cost after iteration 513: 0.299993\n",
      "Cost after iteration 514: 0.299744\n",
      "Cost after iteration 515: 0.299495\n",
      "Cost after iteration 516: 0.299247\n",
      "Cost after iteration 517: 0.298999\n",
      "Cost after iteration 518: 0.298752\n",
      "Cost after iteration 519: 0.298505\n",
      "Cost after iteration 520: 0.298258\n",
      "Cost after iteration 521: 0.298013\n",
      "Cost after iteration 522: 0.297767\n",
      "Cost after iteration 523: 0.297522\n",
      "Cost after iteration 524: 0.297277\n",
      "Cost after iteration 525: 0.297033\n",
      "Cost after iteration 526: 0.296789\n",
      "Cost after iteration 527: 0.296546\n",
      "Cost after iteration 528: 0.296303\n",
      "Cost after iteration 529: 0.296061\n",
      "Cost after iteration 530: 0.295819\n",
      "Cost after iteration 531: 0.295577\n",
      "Cost after iteration 532: 0.295336\n",
      "Cost after iteration 533: 0.295095\n",
      "Cost after iteration 534: 0.294855\n",
      "Cost after iteration 535: 0.294615\n",
      "Cost after iteration 536: 0.294375\n",
      "Cost after iteration 537: 0.294136\n",
      "Cost after iteration 538: 0.293898\n",
      "Cost after iteration 539: 0.293660\n",
      "Cost after iteration 540: 0.293422\n",
      "Cost after iteration 541: 0.293185\n",
      "Cost after iteration 542: 0.292948\n",
      "Cost after iteration 543: 0.292711\n",
      "Cost after iteration 544: 0.292475\n",
      "Cost after iteration 545: 0.292239\n",
      "Cost after iteration 546: 0.292004\n",
      "Cost after iteration 547: 0.291769\n",
      "Cost after iteration 548: 0.291535\n",
      "Cost after iteration 549: 0.291301\n",
      "Cost after iteration 550: 0.291067\n",
      "Cost after iteration 551: 0.290834\n",
      "Cost after iteration 552: 0.290601\n",
      "Cost after iteration 553: 0.290369\n",
      "Cost after iteration 554: 0.290137\n",
      "Cost after iteration 555: 0.289905\n",
      "Cost after iteration 556: 0.289674\n",
      "Cost after iteration 557: 0.289443\n",
      "Cost after iteration 558: 0.289212\n",
      "Cost after iteration 559: 0.288982\n",
      "Cost after iteration 560: 0.288753\n",
      "Cost after iteration 561: 0.288524\n",
      "Cost after iteration 562: 0.288295\n",
      "Cost after iteration 563: 0.288066\n",
      "Cost after iteration 564: 0.287838\n",
      "Cost after iteration 565: 0.287611\n",
      "Cost after iteration 566: 0.287383\n",
      "Cost after iteration 567: 0.287156\n",
      "Cost after iteration 568: 0.286930\n",
      "Cost after iteration 569: 0.286704\n",
      "Cost after iteration 570: 0.286478\n",
      "Cost after iteration 571: 0.286253\n",
      "Cost after iteration 572: 0.286028\n",
      "Cost after iteration 573: 0.285803\n",
      "Cost after iteration 574: 0.285579\n",
      "Cost after iteration 575: 0.285355\n",
      "Cost after iteration 576: 0.285132\n",
      "Cost after iteration 577: 0.284909\n",
      "Cost after iteration 578: 0.284686\n",
      "Cost after iteration 579: 0.284464\n",
      "Cost after iteration 580: 0.284242\n",
      "Cost after iteration 581: 0.284020\n",
      "Cost after iteration 582: 0.283799\n",
      "Cost after iteration 583: 0.283578\n",
      "Cost after iteration 584: 0.283358\n",
      "Cost after iteration 585: 0.283137\n",
      "Cost after iteration 586: 0.282918\n",
      "Cost after iteration 587: 0.282698\n",
      "Cost after iteration 588: 0.282479\n",
      "Cost after iteration 589: 0.282261\n",
      "Cost after iteration 590: 0.282043\n",
      "Cost after iteration 591: 0.281825\n",
      "Cost after iteration 592: 0.281607\n",
      "Cost after iteration 593: 0.281390\n",
      "Cost after iteration 594: 0.281173\n",
      "Cost after iteration 595: 0.280957\n",
      "Cost after iteration 596: 0.280740\n",
      "Cost after iteration 597: 0.280525\n",
      "Cost after iteration 598: 0.280309\n",
      "Cost after iteration 599: 0.280094\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 601: 0.279665\n",
      "Cost after iteration 602: 0.279451\n",
      "Cost after iteration 603: 0.279238\n",
      "Cost after iteration 604: 0.279024\n",
      "Cost after iteration 605: 0.278811\n",
      "Cost after iteration 606: 0.278599\n",
      "Cost after iteration 607: 0.278387\n",
      "Cost after iteration 608: 0.278175\n",
      "Cost after iteration 609: 0.277963\n",
      "Cost after iteration 610: 0.277752\n",
      "Cost after iteration 611: 0.277541\n",
      "Cost after iteration 612: 0.277330\n",
      "Cost after iteration 613: 0.277120\n",
      "Cost after iteration 614: 0.276910\n",
      "Cost after iteration 615: 0.276701\n",
      "Cost after iteration 616: 0.276492\n",
      "Cost after iteration 617: 0.276283\n",
      "Cost after iteration 618: 0.276074\n",
      "Cost after iteration 619: 0.275866\n",
      "Cost after iteration 620: 0.275658\n",
      "Cost after iteration 621: 0.275451\n",
      "Cost after iteration 622: 0.275244\n",
      "Cost after iteration 623: 0.275037\n",
      "Cost after iteration 624: 0.274831\n",
      "Cost after iteration 625: 0.274624\n",
      "Cost after iteration 626: 0.274418\n",
      "Cost after iteration 627: 0.274213\n",
      "Cost after iteration 628: 0.274008\n",
      "Cost after iteration 629: 0.273803\n",
      "Cost after iteration 630: 0.273598\n",
      "Cost after iteration 631: 0.273394\n",
      "Cost after iteration 632: 0.273190\n",
      "Cost after iteration 633: 0.272987\n",
      "Cost after iteration 634: 0.272784\n",
      "Cost after iteration 635: 0.272581\n",
      "Cost after iteration 636: 0.272378\n",
      "Cost after iteration 637: 0.272176\n",
      "Cost after iteration 638: 0.271974\n",
      "Cost after iteration 639: 0.271772\n",
      "Cost after iteration 640: 0.271571\n",
      "Cost after iteration 641: 0.271370\n",
      "Cost after iteration 642: 0.271169\n",
      "Cost after iteration 643: 0.270969\n",
      "Cost after iteration 644: 0.270769\n",
      "Cost after iteration 645: 0.270569\n",
      "Cost after iteration 646: 0.270370\n",
      "Cost after iteration 647: 0.270170\n",
      "Cost after iteration 648: 0.269972\n",
      "Cost after iteration 649: 0.269773\n",
      "Cost after iteration 650: 0.269575\n",
      "Cost after iteration 651: 0.269377\n",
      "Cost after iteration 652: 0.269179\n",
      "Cost after iteration 653: 0.268982\n",
      "Cost after iteration 654: 0.268785\n",
      "Cost after iteration 655: 0.268589\n",
      "Cost after iteration 656: 0.268392\n",
      "Cost after iteration 657: 0.268196\n",
      "Cost after iteration 658: 0.268000\n",
      "Cost after iteration 659: 0.267805\n",
      "Cost after iteration 660: 0.267610\n",
      "Cost after iteration 661: 0.267415\n",
      "Cost after iteration 662: 0.267220\n",
      "Cost after iteration 663: 0.267026\n",
      "Cost after iteration 664: 0.266832\n",
      "Cost after iteration 665: 0.266639\n",
      "Cost after iteration 666: 0.266445\n",
      "Cost after iteration 667: 0.266252\n",
      "Cost after iteration 668: 0.266059\n",
      "Cost after iteration 669: 0.265867\n",
      "Cost after iteration 670: 0.265675\n",
      "Cost after iteration 671: 0.265483\n",
      "Cost after iteration 672: 0.265291\n",
      "Cost after iteration 673: 0.265100\n",
      "Cost after iteration 674: 0.264909\n",
      "Cost after iteration 675: 0.264718\n",
      "Cost after iteration 676: 0.264528\n",
      "Cost after iteration 677: 0.264338\n",
      "Cost after iteration 678: 0.264148\n",
      "Cost after iteration 679: 0.263958\n",
      "Cost after iteration 680: 0.263769\n",
      "Cost after iteration 681: 0.263580\n",
      "Cost after iteration 682: 0.263391\n",
      "Cost after iteration 683: 0.263203\n",
      "Cost after iteration 684: 0.263015\n",
      "Cost after iteration 685: 0.262827\n",
      "Cost after iteration 686: 0.262639\n",
      "Cost after iteration 687: 0.262452\n",
      "Cost after iteration 688: 0.262265\n",
      "Cost after iteration 689: 0.262078\n",
      "Cost after iteration 690: 0.261892\n",
      "Cost after iteration 691: 0.261705\n",
      "Cost after iteration 692: 0.261519\n",
      "Cost after iteration 693: 0.261334\n",
      "Cost after iteration 694: 0.261148\n",
      "Cost after iteration 695: 0.260963\n",
      "Cost after iteration 696: 0.260779\n",
      "Cost after iteration 697: 0.260594\n",
      "Cost after iteration 698: 0.260410\n",
      "Cost after iteration 699: 0.260226\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 701: 0.259859\n",
      "Cost after iteration 702: 0.259676\n",
      "Cost after iteration 703: 0.259493\n",
      "Cost after iteration 704: 0.259310\n",
      "Cost after iteration 705: 0.259128\n",
      "Cost after iteration 706: 0.258946\n",
      "Cost after iteration 707: 0.258764\n",
      "Cost after iteration 708: 0.258582\n",
      "Cost after iteration 709: 0.258401\n",
      "Cost after iteration 710: 0.258220\n",
      "Cost after iteration 711: 0.258039\n",
      "Cost after iteration 712: 0.257859\n",
      "Cost after iteration 713: 0.257678\n",
      "Cost after iteration 714: 0.257498\n",
      "Cost after iteration 715: 0.257319\n",
      "Cost after iteration 716: 0.257139\n",
      "Cost after iteration 717: 0.256960\n",
      "Cost after iteration 718: 0.256781\n",
      "Cost after iteration 719: 0.256602\n",
      "Cost after iteration 720: 0.256424\n",
      "Cost after iteration 721: 0.256246\n",
      "Cost after iteration 722: 0.256068\n",
      "Cost after iteration 723: 0.255890\n",
      "Cost after iteration 724: 0.255713\n",
      "Cost after iteration 725: 0.255536\n",
      "Cost after iteration 726: 0.255359\n",
      "Cost after iteration 727: 0.255182\n",
      "Cost after iteration 728: 0.255006\n",
      "Cost after iteration 729: 0.254830\n",
      "Cost after iteration 730: 0.254654\n",
      "Cost after iteration 731: 0.254479\n",
      "Cost after iteration 732: 0.254303\n",
      "Cost after iteration 733: 0.254128\n",
      "Cost after iteration 734: 0.253953\n",
      "Cost after iteration 735: 0.253779\n",
      "Cost after iteration 736: 0.253604\n",
      "Cost after iteration 737: 0.253430\n",
      "Cost after iteration 738: 0.253257\n",
      "Cost after iteration 739: 0.253083\n",
      "Cost after iteration 740: 0.252910\n",
      "Cost after iteration 741: 0.252736\n",
      "Cost after iteration 742: 0.252564\n",
      "Cost after iteration 743: 0.252391\n",
      "Cost after iteration 744: 0.252219\n",
      "Cost after iteration 745: 0.252047\n",
      "Cost after iteration 746: 0.251875\n",
      "Cost after iteration 747: 0.251703\n",
      "Cost after iteration 748: 0.251532\n",
      "Cost after iteration 749: 0.251361\n",
      "Cost after iteration 750: 0.251190\n",
      "Cost after iteration 751: 0.251019\n",
      "Cost after iteration 752: 0.250849\n",
      "Cost after iteration 753: 0.250678\n",
      "Cost after iteration 754: 0.250508\n",
      "Cost after iteration 755: 0.250339\n",
      "Cost after iteration 756: 0.250169\n",
      "Cost after iteration 757: 0.250000\n",
      "Cost after iteration 758: 0.249831\n",
      "Cost after iteration 759: 0.249662\n",
      "Cost after iteration 760: 0.249494\n",
      "Cost after iteration 761: 0.249325\n",
      "Cost after iteration 762: 0.249157\n",
      "Cost after iteration 763: 0.248990\n",
      "Cost after iteration 764: 0.248822\n",
      "Cost after iteration 765: 0.248655\n",
      "Cost after iteration 766: 0.248488\n",
      "Cost after iteration 767: 0.248321\n",
      "Cost after iteration 768: 0.248154\n",
      "Cost after iteration 769: 0.247988\n",
      "Cost after iteration 770: 0.247821\n",
      "Cost after iteration 771: 0.247656\n",
      "Cost after iteration 772: 0.247490\n",
      "Cost after iteration 773: 0.247324\n",
      "Cost after iteration 774: 0.247159\n",
      "Cost after iteration 775: 0.246994\n",
      "Cost after iteration 776: 0.246829\n",
      "Cost after iteration 777: 0.246665\n",
      "Cost after iteration 778: 0.246500\n",
      "Cost after iteration 779: 0.246336\n",
      "Cost after iteration 780: 0.246172\n",
      "Cost after iteration 781: 0.246009\n",
      "Cost after iteration 782: 0.245845\n",
      "Cost after iteration 783: 0.245682\n",
      "Cost after iteration 784: 0.245519\n",
      "Cost after iteration 785: 0.245356\n",
      "Cost after iteration 786: 0.245193\n",
      "Cost after iteration 787: 0.245031\n",
      "Cost after iteration 788: 0.244869\n",
      "Cost after iteration 789: 0.244707\n",
      "Cost after iteration 790: 0.244545\n",
      "Cost after iteration 791: 0.244384\n",
      "Cost after iteration 792: 0.244223\n",
      "Cost after iteration 793: 0.244062\n",
      "Cost after iteration 794: 0.243901\n",
      "Cost after iteration 795: 0.243740\n",
      "Cost after iteration 796: 0.243580\n",
      "Cost after iteration 797: 0.243420\n",
      "Cost after iteration 798: 0.243260\n",
      "Cost after iteration 799: 0.243100\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 801: 0.242781\n",
      "Cost after iteration 802: 0.242622\n",
      "Cost after iteration 803: 0.242463\n",
      "Cost after iteration 804: 0.242305\n",
      "Cost after iteration 805: 0.242146\n",
      "Cost after iteration 806: 0.241988\n",
      "Cost after iteration 807: 0.241830\n",
      "Cost after iteration 808: 0.241672\n",
      "Cost after iteration 809: 0.241515\n",
      "Cost after iteration 810: 0.241357\n",
      "Cost after iteration 811: 0.241200\n",
      "Cost after iteration 812: 0.241043\n",
      "Cost after iteration 813: 0.240887\n",
      "Cost after iteration 814: 0.240730\n",
      "Cost after iteration 815: 0.240574\n",
      "Cost after iteration 816: 0.240418\n",
      "Cost after iteration 817: 0.240262\n",
      "Cost after iteration 818: 0.240106\n",
      "Cost after iteration 819: 0.239951\n",
      "Cost after iteration 820: 0.239795\n",
      "Cost after iteration 821: 0.239640\n",
      "Cost after iteration 822: 0.239485\n",
      "Cost after iteration 823: 0.239331\n",
      "Cost after iteration 824: 0.239176\n",
      "Cost after iteration 825: 0.239022\n",
      "Cost after iteration 826: 0.238868\n",
      "Cost after iteration 827: 0.238714\n",
      "Cost after iteration 828: 0.238560\n",
      "Cost after iteration 829: 0.238407\n",
      "Cost after iteration 830: 0.238254\n",
      "Cost after iteration 831: 0.238101\n",
      "Cost after iteration 832: 0.237948\n",
      "Cost after iteration 833: 0.237795\n",
      "Cost after iteration 834: 0.237643\n",
      "Cost after iteration 835: 0.237490\n",
      "Cost after iteration 836: 0.237338\n",
      "Cost after iteration 837: 0.237187\n",
      "Cost after iteration 838: 0.237035\n",
      "Cost after iteration 839: 0.236884\n",
      "Cost after iteration 840: 0.236732\n",
      "Cost after iteration 841: 0.236581\n",
      "Cost after iteration 842: 0.236430\n",
      "Cost after iteration 843: 0.236280\n",
      "Cost after iteration 844: 0.236129\n",
      "Cost after iteration 845: 0.235979\n",
      "Cost after iteration 846: 0.235829\n",
      "Cost after iteration 847: 0.235679\n",
      "Cost after iteration 848: 0.235529\n",
      "Cost after iteration 849: 0.235380\n",
      "Cost after iteration 850: 0.235231\n",
      "Cost after iteration 851: 0.235082\n",
      "Cost after iteration 852: 0.234933\n",
      "Cost after iteration 853: 0.234784\n",
      "Cost after iteration 854: 0.234635\n",
      "Cost after iteration 855: 0.234487\n",
      "Cost after iteration 856: 0.234339\n",
      "Cost after iteration 857: 0.234191\n",
      "Cost after iteration 858: 0.234043\n",
      "Cost after iteration 859: 0.233896\n",
      "Cost after iteration 860: 0.233748\n",
      "Cost after iteration 861: 0.233601\n",
      "Cost after iteration 862: 0.233454\n",
      "Cost after iteration 863: 0.233307\n",
      "Cost after iteration 864: 0.233161\n",
      "Cost after iteration 865: 0.233014\n",
      "Cost after iteration 866: 0.232868\n",
      "Cost after iteration 867: 0.232722\n",
      "Cost after iteration 868: 0.232576\n",
      "Cost after iteration 869: 0.232430\n",
      "Cost after iteration 870: 0.232285\n",
      "Cost after iteration 871: 0.232140\n",
      "Cost after iteration 872: 0.231994\n",
      "Cost after iteration 873: 0.231849\n",
      "Cost after iteration 874: 0.231705\n",
      "Cost after iteration 875: 0.231560\n",
      "Cost after iteration 876: 0.231416\n",
      "Cost after iteration 877: 0.231271\n",
      "Cost after iteration 878: 0.231127\n",
      "Cost after iteration 879: 0.230984\n",
      "Cost after iteration 880: 0.230840\n",
      "Cost after iteration 881: 0.230696\n",
      "Cost after iteration 882: 0.230553\n",
      "Cost after iteration 883: 0.230410\n",
      "Cost after iteration 884: 0.230267\n",
      "Cost after iteration 885: 0.230124\n",
      "Cost after iteration 886: 0.229982\n",
      "Cost after iteration 887: 0.229839\n",
      "Cost after iteration 888: 0.229697\n",
      "Cost after iteration 889: 0.229555\n",
      "Cost after iteration 890: 0.229413\n",
      "Cost after iteration 891: 0.229271\n",
      "Cost after iteration 892: 0.229130\n",
      "Cost after iteration 893: 0.228989\n",
      "Cost after iteration 894: 0.228847\n",
      "Cost after iteration 895: 0.228706\n",
      "Cost after iteration 896: 0.228566\n",
      "Cost after iteration 897: 0.228425\n",
      "Cost after iteration 898: 0.228285\n",
      "Cost after iteration 899: 0.228144\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 901: 0.227864\n",
      "Cost after iteration 902: 0.227725\n",
      "Cost after iteration 903: 0.227585\n",
      "Cost after iteration 904: 0.227446\n",
      "Cost after iteration 905: 0.227306\n",
      "Cost after iteration 906: 0.227167\n",
      "Cost after iteration 907: 0.227028\n",
      "Cost after iteration 908: 0.226890\n",
      "Cost after iteration 909: 0.226751\n",
      "Cost after iteration 910: 0.226613\n",
      "Cost after iteration 911: 0.226474\n",
      "Cost after iteration 912: 0.226336\n",
      "Cost after iteration 913: 0.226199\n",
      "Cost after iteration 914: 0.226061\n",
      "Cost after iteration 915: 0.225923\n",
      "Cost after iteration 916: 0.225786\n",
      "Cost after iteration 917: 0.225649\n",
      "Cost after iteration 918: 0.225512\n",
      "Cost after iteration 919: 0.225375\n",
      "Cost after iteration 920: 0.225238\n",
      "Cost after iteration 921: 0.225102\n",
      "Cost after iteration 922: 0.224965\n",
      "Cost after iteration 923: 0.224829\n",
      "Cost after iteration 924: 0.224693\n",
      "Cost after iteration 925: 0.224557\n",
      "Cost after iteration 926: 0.224422\n",
      "Cost after iteration 927: 0.224286\n",
      "Cost after iteration 928: 0.224151\n",
      "Cost after iteration 929: 0.224016\n",
      "Cost after iteration 930: 0.223881\n",
      "Cost after iteration 931: 0.223746\n",
      "Cost after iteration 932: 0.223611\n",
      "Cost after iteration 933: 0.223476\n",
      "Cost after iteration 934: 0.223342\n",
      "Cost after iteration 935: 0.223208\n",
      "Cost after iteration 936: 0.223074\n",
      "Cost after iteration 937: 0.222940\n",
      "Cost after iteration 938: 0.222806\n",
      "Cost after iteration 939: 0.222673\n",
      "Cost after iteration 940: 0.222539\n",
      "Cost after iteration 941: 0.222406\n",
      "Cost after iteration 942: 0.222273\n",
      "Cost after iteration 943: 0.222140\n",
      "Cost after iteration 944: 0.222007\n",
      "Cost after iteration 945: 0.221875\n",
      "Cost after iteration 946: 0.221742\n",
      "Cost after iteration 947: 0.221610\n",
      "Cost after iteration 948: 0.221478\n",
      "Cost after iteration 949: 0.221346\n",
      "Cost after iteration 950: 0.221214\n",
      "Cost after iteration 951: 0.221083\n",
      "Cost after iteration 952: 0.220951\n",
      "Cost after iteration 953: 0.220820\n",
      "Cost after iteration 954: 0.220689\n",
      "Cost after iteration 955: 0.220558\n",
      "Cost after iteration 956: 0.220427\n",
      "Cost after iteration 957: 0.220296\n",
      "Cost after iteration 958: 0.220165\n",
      "Cost after iteration 959: 0.220035\n",
      "Cost after iteration 960: 0.219905\n",
      "Cost after iteration 961: 0.219775\n",
      "Cost after iteration 962: 0.219645\n",
      "Cost after iteration 963: 0.219515\n",
      "Cost after iteration 964: 0.219385\n",
      "Cost after iteration 965: 0.219256\n",
      "Cost after iteration 966: 0.219127\n",
      "Cost after iteration 967: 0.218998\n",
      "Cost after iteration 968: 0.218869\n",
      "Cost after iteration 969: 0.218740\n",
      "Cost after iteration 970: 0.218611\n",
      "Cost after iteration 971: 0.218482\n",
      "Cost after iteration 972: 0.218354\n",
      "Cost after iteration 973: 0.218226\n",
      "Cost after iteration 974: 0.218098\n",
      "Cost after iteration 975: 0.217970\n",
      "Cost after iteration 976: 0.217842\n",
      "Cost after iteration 977: 0.217714\n",
      "Cost after iteration 978: 0.217587\n",
      "Cost after iteration 979: 0.217460\n",
      "Cost after iteration 980: 0.217332\n",
      "Cost after iteration 981: 0.217205\n",
      "Cost after iteration 982: 0.217078\n",
      "Cost after iteration 983: 0.216952\n",
      "Cost after iteration 984: 0.216825\n",
      "Cost after iteration 985: 0.216699\n",
      "Cost after iteration 986: 0.216572\n",
      "Cost after iteration 987: 0.216446\n",
      "Cost after iteration 988: 0.216320\n",
      "Cost after iteration 989: 0.216194\n",
      "Cost after iteration 990: 0.216069\n",
      "Cost after iteration 991: 0.215943\n",
      "Cost after iteration 992: 0.215818\n",
      "Cost after iteration 993: 0.215692\n",
      "Cost after iteration 994: 0.215567\n",
      "Cost after iteration 995: 0.215442\n",
      "Cost after iteration 996: 0.215317\n",
      "Cost after iteration 997: 0.215193\n",
      "Cost after iteration 998: 0.215068\n",
      "Cost after iteration 999: 0.214944\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1001: 0.214695\n",
      "Cost after iteration 1002: 0.214571\n",
      "Cost after iteration 1003: 0.214448\n",
      "Cost after iteration 1004: 0.214324\n",
      "Cost after iteration 1005: 0.214200\n",
      "Cost after iteration 1006: 0.214077\n",
      "Cost after iteration 1007: 0.213954\n",
      "Cost after iteration 1008: 0.213831\n",
      "Cost after iteration 1009: 0.213708\n",
      "Cost after iteration 1010: 0.213585\n",
      "Cost after iteration 1011: 0.213462\n",
      "Cost after iteration 1012: 0.213340\n",
      "Cost after iteration 1013: 0.213217\n",
      "Cost after iteration 1014: 0.213095\n",
      "Cost after iteration 1015: 0.212973\n",
      "Cost after iteration 1016: 0.212851\n",
      "Cost after iteration 1017: 0.212729\n",
      "Cost after iteration 1018: 0.212607\n",
      "Cost after iteration 1019: 0.212486\n",
      "Cost after iteration 1020: 0.212364\n",
      "Cost after iteration 1021: 0.212243\n",
      "Cost after iteration 1022: 0.212122\n",
      "Cost after iteration 1023: 0.212001\n",
      "Cost after iteration 1024: 0.211880\n",
      "Cost after iteration 1025: 0.211759\n",
      "Cost after iteration 1026: 0.211639\n",
      "Cost after iteration 1027: 0.211518\n",
      "Cost after iteration 1028: 0.211398\n",
      "Cost after iteration 1029: 0.211278\n",
      "Cost after iteration 1030: 0.211157\n",
      "Cost after iteration 1031: 0.211038\n",
      "Cost after iteration 1032: 0.210918\n",
      "Cost after iteration 1033: 0.210798\n",
      "Cost after iteration 1034: 0.210679\n",
      "Cost after iteration 1035: 0.210559\n",
      "Cost after iteration 1036: 0.210440\n",
      "Cost after iteration 1037: 0.210321\n",
      "Cost after iteration 1038: 0.210202\n",
      "Cost after iteration 1039: 0.210083\n",
      "Cost after iteration 1040: 0.209964\n",
      "Cost after iteration 1041: 0.209846\n",
      "Cost after iteration 1042: 0.209727\n",
      "Cost after iteration 1043: 0.209609\n",
      "Cost after iteration 1044: 0.209491\n",
      "Cost after iteration 1045: 0.209373\n",
      "Cost after iteration 1046: 0.209255\n",
      "Cost after iteration 1047: 0.209137\n",
      "Cost after iteration 1048: 0.209019\n",
      "Cost after iteration 1049: 0.208902\n",
      "Cost after iteration 1050: 0.208785\n",
      "Cost after iteration 1051: 0.208667\n",
      "Cost after iteration 1052: 0.208550\n",
      "Cost after iteration 1053: 0.208433\n",
      "Cost after iteration 1054: 0.208316\n",
      "Cost after iteration 1055: 0.208200\n",
      "Cost after iteration 1056: 0.208083\n",
      "Cost after iteration 1057: 0.207967\n",
      "Cost after iteration 1058: 0.207850\n",
      "Cost after iteration 1059: 0.207734\n",
      "Cost after iteration 1060: 0.207618\n",
      "Cost after iteration 1061: 0.207502\n",
      "Cost after iteration 1062: 0.207386\n",
      "Cost after iteration 1063: 0.207270\n",
      "Cost after iteration 1064: 0.207155\n",
      "Cost after iteration 1065: 0.207039\n",
      "Cost after iteration 1066: 0.206924\n",
      "Cost after iteration 1067: 0.206809\n",
      "Cost after iteration 1068: 0.206694\n",
      "Cost after iteration 1069: 0.206579\n",
      "Cost after iteration 1070: 0.206464\n",
      "Cost after iteration 1071: 0.206350\n",
      "Cost after iteration 1072: 0.206235\n",
      "Cost after iteration 1073: 0.206121\n",
      "Cost after iteration 1074: 0.206006\n",
      "Cost after iteration 1075: 0.205892\n",
      "Cost after iteration 1076: 0.205778\n",
      "Cost after iteration 1077: 0.205664\n",
      "Cost after iteration 1078: 0.205550\n",
      "Cost after iteration 1079: 0.205437\n",
      "Cost after iteration 1080: 0.205323\n",
      "Cost after iteration 1081: 0.205210\n",
      "Cost after iteration 1082: 0.205096\n",
      "Cost after iteration 1083: 0.204983\n",
      "Cost after iteration 1084: 0.204870\n",
      "Cost after iteration 1085: 0.204757\n",
      "Cost after iteration 1086: 0.204645\n",
      "Cost after iteration 1087: 0.204532\n",
      "Cost after iteration 1088: 0.204419\n",
      "Cost after iteration 1089: 0.204307\n",
      "Cost after iteration 1090: 0.204195\n",
      "Cost after iteration 1091: 0.204082\n",
      "Cost after iteration 1092: 0.203970\n",
      "Cost after iteration 1093: 0.203858\n",
      "Cost after iteration 1094: 0.203747\n",
      "Cost after iteration 1095: 0.203635\n",
      "Cost after iteration 1096: 0.203523\n",
      "Cost after iteration 1097: 0.203412\n",
      "Cost after iteration 1098: 0.203300\n",
      "Cost after iteration 1099: 0.203189\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1101: 0.202967\n",
      "Cost after iteration 1102: 0.202856\n",
      "Cost after iteration 1103: 0.202746\n",
      "Cost after iteration 1104: 0.202635\n",
      "Cost after iteration 1105: 0.202525\n",
      "Cost after iteration 1106: 0.202414\n",
      "Cost after iteration 1107: 0.202304\n",
      "Cost after iteration 1108: 0.202194\n",
      "Cost after iteration 1109: 0.202084\n",
      "Cost after iteration 1110: 0.201974\n",
      "Cost after iteration 1111: 0.201864\n",
      "Cost after iteration 1112: 0.201754\n",
      "Cost after iteration 1113: 0.201645\n",
      "Cost after iteration 1114: 0.201535\n",
      "Cost after iteration 1115: 0.201426\n",
      "Cost after iteration 1116: 0.201317\n",
      "Cost after iteration 1117: 0.201208\n",
      "Cost after iteration 1118: 0.201099\n",
      "Cost after iteration 1119: 0.200990\n",
      "Cost after iteration 1120: 0.200881\n",
      "Cost after iteration 1121: 0.200773\n",
      "Cost after iteration 1122: 0.200664\n",
      "Cost after iteration 1123: 0.200556\n",
      "Cost after iteration 1124: 0.200448\n",
      "Cost after iteration 1125: 0.200339\n",
      "Cost after iteration 1126: 0.200231\n",
      "Cost after iteration 1127: 0.200124\n",
      "Cost after iteration 1128: 0.200016\n",
      "Cost after iteration 1129: 0.199908\n",
      "Cost after iteration 1130: 0.199800\n",
      "Cost after iteration 1131: 0.199693\n",
      "Cost after iteration 1132: 0.199586\n",
      "Cost after iteration 1133: 0.199478\n",
      "Cost after iteration 1134: 0.199371\n",
      "Cost after iteration 1135: 0.199264\n",
      "Cost after iteration 1136: 0.199157\n",
      "Cost after iteration 1137: 0.199051\n",
      "Cost after iteration 1138: 0.198944\n",
      "Cost after iteration 1139: 0.198837\n",
      "Cost after iteration 1140: 0.198731\n",
      "Cost after iteration 1141: 0.198625\n",
      "Cost after iteration 1142: 0.198519\n",
      "Cost after iteration 1143: 0.198412\n",
      "Cost after iteration 1144: 0.198306\n",
      "Cost after iteration 1145: 0.198201\n",
      "Cost after iteration 1146: 0.198095\n",
      "Cost after iteration 1147: 0.197989\n",
      "Cost after iteration 1148: 0.197884\n",
      "Cost after iteration 1149: 0.197778\n",
      "Cost after iteration 1150: 0.197673\n",
      "Cost after iteration 1151: 0.197568\n",
      "Cost after iteration 1152: 0.197463\n",
      "Cost after iteration 1153: 0.197358\n",
      "Cost after iteration 1154: 0.197253\n",
      "Cost after iteration 1155: 0.197148\n",
      "Cost after iteration 1156: 0.197043\n",
      "Cost after iteration 1157: 0.196939\n",
      "Cost after iteration 1158: 0.196834\n",
      "Cost after iteration 1159: 0.196730\n",
      "Cost after iteration 1160: 0.196626\n",
      "Cost after iteration 1161: 0.196522\n",
      "Cost after iteration 1162: 0.196418\n",
      "Cost after iteration 1163: 0.196314\n",
      "Cost after iteration 1164: 0.196210\n",
      "Cost after iteration 1165: 0.196106\n",
      "Cost after iteration 1166: 0.196003\n",
      "Cost after iteration 1167: 0.195899\n",
      "Cost after iteration 1168: 0.195796\n",
      "Cost after iteration 1169: 0.195693\n",
      "Cost after iteration 1170: 0.195589\n",
      "Cost after iteration 1171: 0.195486\n",
      "Cost after iteration 1172: 0.195384\n",
      "Cost after iteration 1173: 0.195281\n",
      "Cost after iteration 1174: 0.195178\n",
      "Cost after iteration 1175: 0.195075\n",
      "Cost after iteration 1176: 0.194973\n",
      "Cost after iteration 1177: 0.194870\n",
      "Cost after iteration 1178: 0.194768\n",
      "Cost after iteration 1179: 0.194666\n",
      "Cost after iteration 1180: 0.194564\n",
      "Cost after iteration 1181: 0.194462\n",
      "Cost after iteration 1182: 0.194360\n",
      "Cost after iteration 1183: 0.194258\n",
      "Cost after iteration 1184: 0.194157\n",
      "Cost after iteration 1185: 0.194055\n",
      "Cost after iteration 1186: 0.193954\n",
      "Cost after iteration 1187: 0.193852\n",
      "Cost after iteration 1188: 0.193751\n",
      "Cost after iteration 1189: 0.193650\n",
      "Cost after iteration 1190: 0.193549\n",
      "Cost after iteration 1191: 0.193448\n",
      "Cost after iteration 1192: 0.193347\n",
      "Cost after iteration 1193: 0.193246\n",
      "Cost after iteration 1194: 0.193146\n",
      "Cost after iteration 1195: 0.193045\n",
      "Cost after iteration 1196: 0.192945\n",
      "Cost after iteration 1197: 0.192845\n",
      "Cost after iteration 1198: 0.192744\n",
      "Cost after iteration 1199: 0.192644\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1201: 0.192444\n",
      "Cost after iteration 1202: 0.192345\n",
      "Cost after iteration 1203: 0.192245\n",
      "Cost after iteration 1204: 0.192145\n",
      "Cost after iteration 1205: 0.192046\n",
      "Cost after iteration 1206: 0.191946\n",
      "Cost after iteration 1207: 0.191847\n",
      "Cost after iteration 1208: 0.191748\n",
      "Cost after iteration 1209: 0.191649\n",
      "Cost after iteration 1210: 0.191550\n",
      "Cost after iteration 1211: 0.191451\n",
      "Cost after iteration 1212: 0.191352\n",
      "Cost after iteration 1213: 0.191253\n",
      "Cost after iteration 1214: 0.191155\n",
      "Cost after iteration 1215: 0.191056\n",
      "Cost after iteration 1216: 0.190958\n",
      "Cost after iteration 1217: 0.190860\n",
      "Cost after iteration 1218: 0.190762\n",
      "Cost after iteration 1219: 0.190663\n",
      "Cost after iteration 1220: 0.190565\n",
      "Cost after iteration 1221: 0.190468\n",
      "Cost after iteration 1222: 0.190370\n",
      "Cost after iteration 1223: 0.190272\n",
      "Cost after iteration 1224: 0.190174\n",
      "Cost after iteration 1225: 0.190077\n",
      "Cost after iteration 1226: 0.189980\n",
      "Cost after iteration 1227: 0.189882\n",
      "Cost after iteration 1228: 0.189785\n",
      "Cost after iteration 1229: 0.189688\n",
      "Cost after iteration 1230: 0.189591\n",
      "Cost after iteration 1231: 0.189494\n",
      "Cost after iteration 1232: 0.189397\n",
      "Cost after iteration 1233: 0.189300\n",
      "Cost after iteration 1234: 0.189204\n",
      "Cost after iteration 1235: 0.189107\n",
      "Cost after iteration 1236: 0.189011\n",
      "Cost after iteration 1237: 0.188915\n",
      "Cost after iteration 1238: 0.188818\n",
      "Cost after iteration 1239: 0.188722\n",
      "Cost after iteration 1240: 0.188626\n",
      "Cost after iteration 1241: 0.188530\n",
      "Cost after iteration 1242: 0.188434\n",
      "Cost after iteration 1243: 0.188339\n",
      "Cost after iteration 1244: 0.188243\n",
      "Cost after iteration 1245: 0.188147\n",
      "Cost after iteration 1246: 0.188052\n",
      "Cost after iteration 1247: 0.187956\n",
      "Cost after iteration 1248: 0.187861\n",
      "Cost after iteration 1249: 0.187766\n",
      "Cost after iteration 1250: 0.187671\n",
      "Cost after iteration 1251: 0.187576\n",
      "Cost after iteration 1252: 0.187481\n",
      "Cost after iteration 1253: 0.187386\n",
      "Cost after iteration 1254: 0.187291\n",
      "Cost after iteration 1255: 0.187197\n",
      "Cost after iteration 1256: 0.187102\n",
      "Cost after iteration 1257: 0.187008\n",
      "Cost after iteration 1258: 0.186913\n",
      "Cost after iteration 1259: 0.186819\n",
      "Cost after iteration 1260: 0.186725\n",
      "Cost after iteration 1261: 0.186631\n",
      "Cost after iteration 1262: 0.186537\n",
      "Cost after iteration 1263: 0.186443\n",
      "Cost after iteration 1264: 0.186349\n",
      "Cost after iteration 1265: 0.186256\n",
      "Cost after iteration 1266: 0.186162\n",
      "Cost after iteration 1267: 0.186069\n",
      "Cost after iteration 1268: 0.185975\n",
      "Cost after iteration 1269: 0.185882\n",
      "Cost after iteration 1270: 0.185789\n",
      "Cost after iteration 1271: 0.185695\n",
      "Cost after iteration 1272: 0.185602\n",
      "Cost after iteration 1273: 0.185509\n",
      "Cost after iteration 1274: 0.185417\n",
      "Cost after iteration 1275: 0.185324\n",
      "Cost after iteration 1276: 0.185231\n",
      "Cost after iteration 1277: 0.185138\n",
      "Cost after iteration 1278: 0.185046\n",
      "Cost after iteration 1279: 0.184954\n",
      "Cost after iteration 1280: 0.184861\n",
      "Cost after iteration 1281: 0.184769\n",
      "Cost after iteration 1282: 0.184677\n",
      "Cost after iteration 1283: 0.184585\n",
      "Cost after iteration 1284: 0.184493\n",
      "Cost after iteration 1285: 0.184401\n",
      "Cost after iteration 1286: 0.184309\n",
      "Cost after iteration 1287: 0.184217\n",
      "Cost after iteration 1288: 0.184126\n",
      "Cost after iteration 1289: 0.184034\n",
      "Cost after iteration 1290: 0.183943\n",
      "Cost after iteration 1291: 0.183851\n",
      "Cost after iteration 1292: 0.183760\n",
      "Cost after iteration 1293: 0.183669\n",
      "Cost after iteration 1294: 0.183578\n",
      "Cost after iteration 1295: 0.183487\n",
      "Cost after iteration 1296: 0.183396\n",
      "Cost after iteration 1297: 0.183305\n",
      "Cost after iteration 1298: 0.183215\n",
      "Cost after iteration 1299: 0.183124\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1301: 0.182943\n",
      "Cost after iteration 1302: 0.182852\n",
      "Cost after iteration 1303: 0.182762\n",
      "Cost after iteration 1304: 0.182672\n",
      "Cost after iteration 1305: 0.182582\n",
      "Cost after iteration 1306: 0.182492\n",
      "Cost after iteration 1307: 0.182402\n",
      "Cost after iteration 1308: 0.182312\n",
      "Cost after iteration 1309: 0.182222\n",
      "Cost after iteration 1310: 0.182133\n",
      "Cost after iteration 1311: 0.182043\n",
      "Cost after iteration 1312: 0.181953\n",
      "Cost after iteration 1313: 0.181864\n",
      "Cost after iteration 1314: 0.181775\n",
      "Cost after iteration 1315: 0.181685\n",
      "Cost after iteration 1316: 0.181596\n",
      "Cost after iteration 1317: 0.181507\n",
      "Cost after iteration 1318: 0.181418\n",
      "Cost after iteration 1319: 0.181329\n",
      "Cost after iteration 1320: 0.181240\n",
      "Cost after iteration 1321: 0.181152\n",
      "Cost after iteration 1322: 0.181063\n",
      "Cost after iteration 1323: 0.180974\n",
      "Cost after iteration 1324: 0.180886\n",
      "Cost after iteration 1325: 0.180798\n",
      "Cost after iteration 1326: 0.180709\n",
      "Cost after iteration 1327: 0.180621\n",
      "Cost after iteration 1328: 0.180533\n",
      "Cost after iteration 1329: 0.180445\n",
      "Cost after iteration 1330: 0.180357\n",
      "Cost after iteration 1331: 0.180269\n",
      "Cost after iteration 1332: 0.180181\n",
      "Cost after iteration 1333: 0.180093\n",
      "Cost after iteration 1334: 0.180006\n",
      "Cost after iteration 1335: 0.179918\n",
      "Cost after iteration 1336: 0.179831\n",
      "Cost after iteration 1337: 0.179743\n",
      "Cost after iteration 1338: 0.179656\n",
      "Cost after iteration 1339: 0.179569\n",
      "Cost after iteration 1340: 0.179481\n",
      "Cost after iteration 1341: 0.179394\n",
      "Cost after iteration 1342: 0.179307\n",
      "Cost after iteration 1343: 0.179221\n",
      "Cost after iteration 1344: 0.179134\n",
      "Cost after iteration 1345: 0.179047\n",
      "Cost after iteration 1346: 0.178960\n",
      "Cost after iteration 1347: 0.178874\n",
      "Cost after iteration 1348: 0.178787\n",
      "Cost after iteration 1349: 0.178701\n",
      "Cost after iteration 1350: 0.178614\n",
      "Cost after iteration 1351: 0.178528\n",
      "Cost after iteration 1352: 0.178442\n",
      "Cost after iteration 1353: 0.178356\n",
      "Cost after iteration 1354: 0.178270\n",
      "Cost after iteration 1355: 0.178184\n",
      "Cost after iteration 1356: 0.178098\n",
      "Cost after iteration 1357: 0.178012\n",
      "Cost after iteration 1358: 0.177927\n",
      "Cost after iteration 1359: 0.177841\n",
      "Cost after iteration 1360: 0.177755\n",
      "Cost after iteration 1361: 0.177670\n",
      "Cost after iteration 1362: 0.177585\n",
      "Cost after iteration 1363: 0.177499\n",
      "Cost after iteration 1364: 0.177414\n",
      "Cost after iteration 1365: 0.177329\n",
      "Cost after iteration 1366: 0.177244\n",
      "Cost after iteration 1367: 0.177159\n",
      "Cost after iteration 1368: 0.177074\n",
      "Cost after iteration 1369: 0.176989\n",
      "Cost after iteration 1370: 0.176905\n",
      "Cost after iteration 1371: 0.176820\n",
      "Cost after iteration 1372: 0.176735\n",
      "Cost after iteration 1373: 0.176651\n",
      "Cost after iteration 1374: 0.176566\n",
      "Cost after iteration 1375: 0.176482\n",
      "Cost after iteration 1376: 0.176398\n",
      "Cost after iteration 1377: 0.176314\n",
      "Cost after iteration 1378: 0.176229\n",
      "Cost after iteration 1379: 0.176145\n",
      "Cost after iteration 1380: 0.176061\n",
      "Cost after iteration 1381: 0.175978\n",
      "Cost after iteration 1382: 0.175894\n",
      "Cost after iteration 1383: 0.175810\n",
      "Cost after iteration 1384: 0.175726\n",
      "Cost after iteration 1385: 0.175643\n",
      "Cost after iteration 1386: 0.175559\n",
      "Cost after iteration 1387: 0.175476\n",
      "Cost after iteration 1388: 0.175393\n",
      "Cost after iteration 1389: 0.175309\n",
      "Cost after iteration 1390: 0.175226\n",
      "Cost after iteration 1391: 0.175143\n",
      "Cost after iteration 1392: 0.175060\n",
      "Cost after iteration 1393: 0.174977\n",
      "Cost after iteration 1394: 0.174894\n",
      "Cost after iteration 1395: 0.174811\n",
      "Cost after iteration 1396: 0.174729\n",
      "Cost after iteration 1397: 0.174646\n",
      "Cost after iteration 1398: 0.174564\n",
      "Cost after iteration 1399: 0.174481\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1401: 0.174316\n",
      "Cost after iteration 1402: 0.174234\n",
      "Cost after iteration 1403: 0.174152\n",
      "Cost after iteration 1404: 0.174070\n",
      "Cost after iteration 1405: 0.173988\n",
      "Cost after iteration 1406: 0.173906\n",
      "Cost after iteration 1407: 0.173824\n",
      "Cost after iteration 1408: 0.173742\n",
      "Cost after iteration 1409: 0.173660\n",
      "Cost after iteration 1410: 0.173579\n",
      "Cost after iteration 1411: 0.173497\n",
      "Cost after iteration 1412: 0.173415\n",
      "Cost after iteration 1413: 0.173334\n",
      "Cost after iteration 1414: 0.173253\n",
      "Cost after iteration 1415: 0.173171\n",
      "Cost after iteration 1416: 0.173090\n",
      "Cost after iteration 1417: 0.173009\n",
      "Cost after iteration 1418: 0.172928\n",
      "Cost after iteration 1419: 0.172847\n",
      "Cost after iteration 1420: 0.172766\n",
      "Cost after iteration 1421: 0.172685\n",
      "Cost after iteration 1422: 0.172604\n",
      "Cost after iteration 1423: 0.172524\n",
      "Cost after iteration 1424: 0.172443\n",
      "Cost after iteration 1425: 0.172362\n",
      "Cost after iteration 1426: 0.172282\n",
      "Cost after iteration 1427: 0.172201\n",
      "Cost after iteration 1428: 0.172121\n",
      "Cost after iteration 1429: 0.172041\n",
      "Cost after iteration 1430: 0.171961\n",
      "Cost after iteration 1431: 0.171881\n",
      "Cost after iteration 1432: 0.171801\n",
      "Cost after iteration 1433: 0.171721\n",
      "Cost after iteration 1434: 0.171641\n",
      "Cost after iteration 1435: 0.171561\n",
      "Cost after iteration 1436: 0.171481\n",
      "Cost after iteration 1437: 0.171401\n",
      "Cost after iteration 1438: 0.171322\n",
      "Cost after iteration 1439: 0.171242\n",
      "Cost after iteration 1440: 0.171163\n",
      "Cost after iteration 1441: 0.171083\n",
      "Cost after iteration 1442: 0.171004\n",
      "Cost after iteration 1443: 0.170925\n",
      "Cost after iteration 1444: 0.170845\n",
      "Cost after iteration 1445: 0.170766\n",
      "Cost after iteration 1446: 0.170687\n",
      "Cost after iteration 1447: 0.170608\n",
      "Cost after iteration 1448: 0.170529\n",
      "Cost after iteration 1449: 0.170451\n",
      "Cost after iteration 1450: 0.170372\n",
      "Cost after iteration 1451: 0.170293\n",
      "Cost after iteration 1452: 0.170214\n",
      "Cost after iteration 1453: 0.170136\n",
      "Cost after iteration 1454: 0.170057\n",
      "Cost after iteration 1455: 0.169979\n",
      "Cost after iteration 1456: 0.169901\n",
      "Cost after iteration 1457: 0.169822\n",
      "Cost after iteration 1458: 0.169744\n",
      "Cost after iteration 1459: 0.169666\n",
      "Cost after iteration 1460: 0.169588\n",
      "Cost after iteration 1461: 0.169510\n",
      "Cost after iteration 1462: 0.169432\n",
      "Cost after iteration 1463: 0.169354\n",
      "Cost after iteration 1464: 0.169276\n",
      "Cost after iteration 1465: 0.169199\n",
      "Cost after iteration 1466: 0.169121\n",
      "Cost after iteration 1467: 0.169043\n",
      "Cost after iteration 1468: 0.168966\n",
      "Cost after iteration 1469: 0.168889\n",
      "Cost after iteration 1470: 0.168811\n",
      "Cost after iteration 1471: 0.168734\n",
      "Cost after iteration 1472: 0.168657\n",
      "Cost after iteration 1473: 0.168579\n",
      "Cost after iteration 1474: 0.168502\n",
      "Cost after iteration 1475: 0.168425\n",
      "Cost after iteration 1476: 0.168348\n",
      "Cost after iteration 1477: 0.168271\n",
      "Cost after iteration 1478: 0.168195\n",
      "Cost after iteration 1479: 0.168118\n",
      "Cost after iteration 1480: 0.168041\n",
      "Cost after iteration 1481: 0.167965\n",
      "Cost after iteration 1482: 0.167888\n",
      "Cost after iteration 1483: 0.167811\n",
      "Cost after iteration 1484: 0.167735\n",
      "Cost after iteration 1485: 0.167659\n",
      "Cost after iteration 1486: 0.167582\n",
      "Cost after iteration 1487: 0.167506\n",
      "Cost after iteration 1488: 0.167430\n",
      "Cost after iteration 1489: 0.167354\n",
      "Cost after iteration 1490: 0.167278\n",
      "Cost after iteration 1491: 0.167202\n",
      "Cost after iteration 1492: 0.167126\n",
      "Cost after iteration 1493: 0.167050\n",
      "Cost after iteration 1494: 0.166975\n",
      "Cost after iteration 1495: 0.166899\n",
      "Cost after iteration 1496: 0.166823\n",
      "Cost after iteration 1497: 0.166748\n",
      "Cost after iteration 1498: 0.166672\n",
      "Cost after iteration 1499: 0.166597\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1501: 0.166446\n",
      "Cost after iteration 1502: 0.166371\n",
      "Cost after iteration 1503: 0.166296\n",
      "Cost after iteration 1504: 0.166221\n",
      "Cost after iteration 1505: 0.166146\n",
      "Cost after iteration 1506: 0.166071\n",
      "Cost after iteration 1507: 0.165996\n",
      "Cost after iteration 1508: 0.165921\n",
      "Cost after iteration 1509: 0.165846\n",
      "Cost after iteration 1510: 0.165771\n",
      "Cost after iteration 1511: 0.165697\n",
      "Cost after iteration 1512: 0.165622\n",
      "Cost after iteration 1513: 0.165548\n",
      "Cost after iteration 1514: 0.165473\n",
      "Cost after iteration 1515: 0.165399\n",
      "Cost after iteration 1516: 0.165325\n",
      "Cost after iteration 1517: 0.165250\n",
      "Cost after iteration 1518: 0.165176\n",
      "Cost after iteration 1519: 0.165102\n",
      "Cost after iteration 1520: 0.165028\n",
      "Cost after iteration 1521: 0.164954\n",
      "Cost after iteration 1522: 0.164880\n",
      "Cost after iteration 1523: 0.164806\n",
      "Cost after iteration 1524: 0.164732\n",
      "Cost after iteration 1525: 0.164659\n",
      "Cost after iteration 1526: 0.164585\n",
      "Cost after iteration 1527: 0.164511\n",
      "Cost after iteration 1528: 0.164438\n",
      "Cost after iteration 1529: 0.164364\n",
      "Cost after iteration 1530: 0.164291\n",
      "Cost after iteration 1531: 0.164218\n",
      "Cost after iteration 1532: 0.164144\n",
      "Cost after iteration 1533: 0.164071\n",
      "Cost after iteration 1534: 0.163998\n",
      "Cost after iteration 1535: 0.163925\n",
      "Cost after iteration 1536: 0.163852\n",
      "Cost after iteration 1537: 0.163779\n",
      "Cost after iteration 1538: 0.163706\n",
      "Cost after iteration 1539: 0.163633\n",
      "Cost after iteration 1540: 0.163560\n",
      "Cost after iteration 1541: 0.163488\n",
      "Cost after iteration 1542: 0.163415\n",
      "Cost after iteration 1543: 0.163342\n",
      "Cost after iteration 1544: 0.163270\n",
      "Cost after iteration 1545: 0.163197\n",
      "Cost after iteration 1546: 0.163125\n",
      "Cost after iteration 1547: 0.163052\n",
      "Cost after iteration 1548: 0.162980\n",
      "Cost after iteration 1549: 0.162908\n",
      "Cost after iteration 1550: 0.162836\n",
      "Cost after iteration 1551: 0.162764\n",
      "Cost after iteration 1552: 0.162692\n",
      "Cost after iteration 1553: 0.162620\n",
      "Cost after iteration 1554: 0.162548\n",
      "Cost after iteration 1555: 0.162476\n",
      "Cost after iteration 1556: 0.162404\n",
      "Cost after iteration 1557: 0.162332\n",
      "Cost after iteration 1558: 0.162261\n",
      "Cost after iteration 1559: 0.162189\n",
      "Cost after iteration 1560: 0.162118\n",
      "Cost after iteration 1561: 0.162046\n",
      "Cost after iteration 1562: 0.161975\n",
      "Cost after iteration 1563: 0.161903\n",
      "Cost after iteration 1564: 0.161832\n",
      "Cost after iteration 1565: 0.161761\n",
      "Cost after iteration 1566: 0.161689\n",
      "Cost after iteration 1567: 0.161618\n",
      "Cost after iteration 1568: 0.161547\n",
      "Cost after iteration 1569: 0.161476\n",
      "Cost after iteration 1570: 0.161405\n",
      "Cost after iteration 1571: 0.161334\n",
      "Cost after iteration 1572: 0.161264\n",
      "Cost after iteration 1573: 0.161193\n",
      "Cost after iteration 1574: 0.161122\n",
      "Cost after iteration 1575: 0.161051\n",
      "Cost after iteration 1576: 0.160981\n",
      "Cost after iteration 1577: 0.160910\n",
      "Cost after iteration 1578: 0.160840\n",
      "Cost after iteration 1579: 0.160769\n",
      "Cost after iteration 1580: 0.160699\n",
      "Cost after iteration 1581: 0.160629\n",
      "Cost after iteration 1582: 0.160559\n",
      "Cost after iteration 1583: 0.160488\n",
      "Cost after iteration 1584: 0.160418\n",
      "Cost after iteration 1585: 0.160348\n",
      "Cost after iteration 1586: 0.160278\n",
      "Cost after iteration 1587: 0.160208\n",
      "Cost after iteration 1588: 0.160138\n",
      "Cost after iteration 1589: 0.160069\n",
      "Cost after iteration 1590: 0.159999\n",
      "Cost after iteration 1591: 0.159929\n",
      "Cost after iteration 1592: 0.159860\n",
      "Cost after iteration 1593: 0.159790\n",
      "Cost after iteration 1594: 0.159720\n",
      "Cost after iteration 1595: 0.159651\n",
      "Cost after iteration 1596: 0.159582\n",
      "Cost after iteration 1597: 0.159512\n",
      "Cost after iteration 1598: 0.159443\n",
      "Cost after iteration 1599: 0.159374\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1601: 0.159235\n",
      "Cost after iteration 1602: 0.159166\n",
      "Cost after iteration 1603: 0.159097\n",
      "Cost after iteration 1604: 0.159028\n",
      "Cost after iteration 1605: 0.158960\n",
      "Cost after iteration 1606: 0.158891\n",
      "Cost after iteration 1607: 0.158822\n",
      "Cost after iteration 1608: 0.158753\n",
      "Cost after iteration 1609: 0.158685\n",
      "Cost after iteration 1610: 0.158616\n",
      "Cost after iteration 1611: 0.158547\n",
      "Cost after iteration 1612: 0.158479\n",
      "Cost after iteration 1613: 0.158410\n",
      "Cost after iteration 1614: 0.158342\n",
      "Cost after iteration 1615: 0.158274\n",
      "Cost after iteration 1616: 0.158206\n",
      "Cost after iteration 1617: 0.158137\n",
      "Cost after iteration 1618: 0.158069\n",
      "Cost after iteration 1619: 0.158001\n",
      "Cost after iteration 1620: 0.157933\n",
      "Cost after iteration 1621: 0.157865\n",
      "Cost after iteration 1622: 0.157797\n",
      "Cost after iteration 1623: 0.157729\n",
      "Cost after iteration 1624: 0.157661\n",
      "Cost after iteration 1625: 0.157594\n",
      "Cost after iteration 1626: 0.157526\n",
      "Cost after iteration 1627: 0.157458\n",
      "Cost after iteration 1628: 0.157391\n",
      "Cost after iteration 1629: 0.157323\n",
      "Cost after iteration 1630: 0.157256\n",
      "Cost after iteration 1631: 0.157188\n",
      "Cost after iteration 1632: 0.157121\n",
      "Cost after iteration 1633: 0.157054\n",
      "Cost after iteration 1634: 0.156986\n",
      "Cost after iteration 1635: 0.156919\n",
      "Cost after iteration 1636: 0.156852\n",
      "Cost after iteration 1637: 0.156785\n",
      "Cost after iteration 1638: 0.156718\n",
      "Cost after iteration 1639: 0.156651\n",
      "Cost after iteration 1640: 0.156584\n",
      "Cost after iteration 1641: 0.156517\n",
      "Cost after iteration 1642: 0.156450\n",
      "Cost after iteration 1643: 0.156384\n",
      "Cost after iteration 1644: 0.156317\n",
      "Cost after iteration 1645: 0.156250\n",
      "Cost after iteration 1646: 0.156184\n",
      "Cost after iteration 1647: 0.156117\n",
      "Cost after iteration 1648: 0.156051\n",
      "Cost after iteration 1649: 0.155984\n",
      "Cost after iteration 1650: 0.155918\n",
      "Cost after iteration 1651: 0.155852\n",
      "Cost after iteration 1652: 0.155785\n",
      "Cost after iteration 1653: 0.155719\n",
      "Cost after iteration 1654: 0.155653\n",
      "Cost after iteration 1655: 0.155587\n",
      "Cost after iteration 1656: 0.155521\n",
      "Cost after iteration 1657: 0.155455\n",
      "Cost after iteration 1658: 0.155389\n",
      "Cost after iteration 1659: 0.155323\n",
      "Cost after iteration 1660: 0.155257\n",
      "Cost after iteration 1661: 0.155191\n",
      "Cost after iteration 1662: 0.155126\n",
      "Cost after iteration 1663: 0.155060\n",
      "Cost after iteration 1664: 0.154994\n",
      "Cost after iteration 1665: 0.154929\n",
      "Cost after iteration 1666: 0.154863\n",
      "Cost after iteration 1667: 0.154798\n",
      "Cost after iteration 1668: 0.154733\n",
      "Cost after iteration 1669: 0.154667\n",
      "Cost after iteration 1670: 0.154602\n",
      "Cost after iteration 1671: 0.154537\n",
      "Cost after iteration 1672: 0.154471\n",
      "Cost after iteration 1673: 0.154406\n",
      "Cost after iteration 1674: 0.154341\n",
      "Cost after iteration 1675: 0.154276\n",
      "Cost after iteration 1676: 0.154211\n",
      "Cost after iteration 1677: 0.154146\n",
      "Cost after iteration 1678: 0.154081\n",
      "Cost after iteration 1679: 0.154017\n",
      "Cost after iteration 1680: 0.153952\n",
      "Cost after iteration 1681: 0.153887\n",
      "Cost after iteration 1682: 0.153822\n",
      "Cost after iteration 1683: 0.153758\n",
      "Cost after iteration 1684: 0.153693\n",
      "Cost after iteration 1685: 0.153629\n",
      "Cost after iteration 1686: 0.153564\n",
      "Cost after iteration 1687: 0.153500\n",
      "Cost after iteration 1688: 0.153436\n",
      "Cost after iteration 1689: 0.153371\n",
      "Cost after iteration 1690: 0.153307\n",
      "Cost after iteration 1691: 0.153243\n",
      "Cost after iteration 1692: 0.153179\n",
      "Cost after iteration 1693: 0.153115\n",
      "Cost after iteration 1694: 0.153050\n",
      "Cost after iteration 1695: 0.152987\n",
      "Cost after iteration 1696: 0.152923\n",
      "Cost after iteration 1697: 0.152859\n",
      "Cost after iteration 1698: 0.152795\n",
      "Cost after iteration 1699: 0.152731\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1701: 0.152604\n",
      "Cost after iteration 1702: 0.152540\n",
      "Cost after iteration 1703: 0.152476\n",
      "Cost after iteration 1704: 0.152413\n",
      "Cost after iteration 1705: 0.152349\n",
      "Cost after iteration 1706: 0.152286\n",
      "Cost after iteration 1707: 0.152223\n",
      "Cost after iteration 1708: 0.152159\n",
      "Cost after iteration 1709: 0.152096\n",
      "Cost after iteration 1710: 0.152033\n",
      "Cost after iteration 1711: 0.151970\n",
      "Cost after iteration 1712: 0.151906\n",
      "Cost after iteration 1713: 0.151843\n",
      "Cost after iteration 1714: 0.151780\n",
      "Cost after iteration 1715: 0.151717\n",
      "Cost after iteration 1716: 0.151654\n",
      "Cost after iteration 1717: 0.151592\n",
      "Cost after iteration 1718: 0.151529\n",
      "Cost after iteration 1719: 0.151466\n",
      "Cost after iteration 1720: 0.151403\n",
      "Cost after iteration 1721: 0.151341\n",
      "Cost after iteration 1722: 0.151278\n",
      "Cost after iteration 1723: 0.151215\n",
      "Cost after iteration 1724: 0.151153\n",
      "Cost after iteration 1725: 0.151090\n",
      "Cost after iteration 1726: 0.151028\n",
      "Cost after iteration 1727: 0.150966\n",
      "Cost after iteration 1728: 0.150903\n",
      "Cost after iteration 1729: 0.150841\n",
      "Cost after iteration 1730: 0.150779\n",
      "Cost after iteration 1731: 0.150717\n",
      "Cost after iteration 1732: 0.150655\n",
      "Cost after iteration 1733: 0.150592\n",
      "Cost after iteration 1734: 0.150530\n",
      "Cost after iteration 1735: 0.150468\n",
      "Cost after iteration 1736: 0.150406\n",
      "Cost after iteration 1737: 0.150345\n",
      "Cost after iteration 1738: 0.150283\n",
      "Cost after iteration 1739: 0.150221\n",
      "Cost after iteration 1740: 0.150159\n",
      "Cost after iteration 1741: 0.150098\n",
      "Cost after iteration 1742: 0.150036\n",
      "Cost after iteration 1743: 0.149974\n",
      "Cost after iteration 1744: 0.149913\n",
      "Cost after iteration 1745: 0.149851\n",
      "Cost after iteration 1746: 0.149790\n",
      "Cost after iteration 1747: 0.149728\n",
      "Cost after iteration 1748: 0.149667\n",
      "Cost after iteration 1749: 0.149606\n",
      "Cost after iteration 1750: 0.149545\n",
      "Cost after iteration 1751: 0.149483\n",
      "Cost after iteration 1752: 0.149422\n",
      "Cost after iteration 1753: 0.149361\n",
      "Cost after iteration 1754: 0.149300\n",
      "Cost after iteration 1755: 0.149239\n",
      "Cost after iteration 1756: 0.149178\n",
      "Cost after iteration 1757: 0.149117\n",
      "Cost after iteration 1758: 0.149056\n",
      "Cost after iteration 1759: 0.148995\n",
      "Cost after iteration 1760: 0.148935\n",
      "Cost after iteration 1761: 0.148874\n",
      "Cost after iteration 1762: 0.148813\n",
      "Cost after iteration 1763: 0.148753\n",
      "Cost after iteration 1764: 0.148692\n",
      "Cost after iteration 1765: 0.148632\n",
      "Cost after iteration 1766: 0.148571\n",
      "Cost after iteration 1767: 0.148511\n",
      "Cost after iteration 1768: 0.148450\n",
      "Cost after iteration 1769: 0.148390\n",
      "Cost after iteration 1770: 0.148330\n",
      "Cost after iteration 1771: 0.148269\n",
      "Cost after iteration 1772: 0.148209\n",
      "Cost after iteration 1773: 0.148149\n",
      "Cost after iteration 1774: 0.148089\n",
      "Cost after iteration 1775: 0.148029\n",
      "Cost after iteration 1776: 0.147969\n",
      "Cost after iteration 1777: 0.147909\n",
      "Cost after iteration 1778: 0.147849\n",
      "Cost after iteration 1779: 0.147789\n",
      "Cost after iteration 1780: 0.147729\n",
      "Cost after iteration 1781: 0.147669\n",
      "Cost after iteration 1782: 0.147610\n",
      "Cost after iteration 1783: 0.147550\n",
      "Cost after iteration 1784: 0.147490\n",
      "Cost after iteration 1785: 0.147431\n",
      "Cost after iteration 1786: 0.147371\n",
      "Cost after iteration 1787: 0.147312\n",
      "Cost after iteration 1788: 0.147252\n",
      "Cost after iteration 1789: 0.147193\n",
      "Cost after iteration 1790: 0.147133\n",
      "Cost after iteration 1791: 0.147074\n",
      "Cost after iteration 1792: 0.147015\n",
      "Cost after iteration 1793: 0.146956\n",
      "Cost after iteration 1794: 0.146896\n",
      "Cost after iteration 1795: 0.146837\n",
      "Cost after iteration 1796: 0.146778\n",
      "Cost after iteration 1797: 0.146719\n",
      "Cost after iteration 1798: 0.146660\n",
      "Cost after iteration 1799: 0.146601\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1801: 0.146483\n",
      "Cost after iteration 1802: 0.146425\n",
      "Cost after iteration 1803: 0.146366\n",
      "Cost after iteration 1804: 0.146307\n",
      "Cost after iteration 1805: 0.146248\n",
      "Cost after iteration 1806: 0.146190\n",
      "Cost after iteration 1807: 0.146131\n",
      "Cost after iteration 1808: 0.146073\n",
      "Cost after iteration 1809: 0.146014\n",
      "Cost after iteration 1810: 0.145956\n",
      "Cost after iteration 1811: 0.145897\n",
      "Cost after iteration 1812: 0.145839\n",
      "Cost after iteration 1813: 0.145781\n",
      "Cost after iteration 1814: 0.145722\n",
      "Cost after iteration 1815: 0.145664\n",
      "Cost after iteration 1816: 0.145606\n",
      "Cost after iteration 1817: 0.145548\n",
      "Cost after iteration 1818: 0.145490\n",
      "Cost after iteration 1819: 0.145431\n",
      "Cost after iteration 1820: 0.145373\n",
      "Cost after iteration 1821: 0.145316\n",
      "Cost after iteration 1822: 0.145258\n",
      "Cost after iteration 1823: 0.145200\n",
      "Cost after iteration 1824: 0.145142\n",
      "Cost after iteration 1825: 0.145084\n",
      "Cost after iteration 1826: 0.145026\n",
      "Cost after iteration 1827: 0.144969\n",
      "Cost after iteration 1828: 0.144911\n",
      "Cost after iteration 1829: 0.144853\n",
      "Cost after iteration 1830: 0.144796\n",
      "Cost after iteration 1831: 0.144738\n",
      "Cost after iteration 1832: 0.144681\n",
      "Cost after iteration 1833: 0.144623\n",
      "Cost after iteration 1834: 0.144566\n",
      "Cost after iteration 1835: 0.144509\n",
      "Cost after iteration 1836: 0.144451\n",
      "Cost after iteration 1837: 0.144394\n",
      "Cost after iteration 1838: 0.144337\n",
      "Cost after iteration 1839: 0.144280\n",
      "Cost after iteration 1840: 0.144222\n",
      "Cost after iteration 1841: 0.144165\n",
      "Cost after iteration 1842: 0.144108\n",
      "Cost after iteration 1843: 0.144051\n",
      "Cost after iteration 1844: 0.143994\n",
      "Cost after iteration 1845: 0.143937\n",
      "Cost after iteration 1846: 0.143881\n",
      "Cost after iteration 1847: 0.143824\n",
      "Cost after iteration 1848: 0.143767\n",
      "Cost after iteration 1849: 0.143710\n",
      "Cost after iteration 1850: 0.143654\n",
      "Cost after iteration 1851: 0.143597\n",
      "Cost after iteration 1852: 0.143540\n",
      "Cost after iteration 1853: 0.143484\n",
      "Cost after iteration 1854: 0.143427\n",
      "Cost after iteration 1855: 0.143371\n",
      "Cost after iteration 1856: 0.143314\n",
      "Cost after iteration 1857: 0.143258\n",
      "Cost after iteration 1858: 0.143201\n",
      "Cost after iteration 1859: 0.143145\n",
      "Cost after iteration 1860: 0.143089\n",
      "Cost after iteration 1861: 0.143033\n",
      "Cost after iteration 1862: 0.142976\n",
      "Cost after iteration 1863: 0.142920\n",
      "Cost after iteration 1864: 0.142864\n",
      "Cost after iteration 1865: 0.142808\n",
      "Cost after iteration 1866: 0.142752\n",
      "Cost after iteration 1867: 0.142696\n",
      "Cost after iteration 1868: 0.142640\n",
      "Cost after iteration 1869: 0.142584\n",
      "Cost after iteration 1870: 0.142528\n",
      "Cost after iteration 1871: 0.142473\n",
      "Cost after iteration 1872: 0.142417\n",
      "Cost after iteration 1873: 0.142361\n",
      "Cost after iteration 1874: 0.142305\n",
      "Cost after iteration 1875: 0.142250\n",
      "Cost after iteration 1876: 0.142194\n",
      "Cost after iteration 1877: 0.142139\n",
      "Cost after iteration 1878: 0.142083\n",
      "Cost after iteration 1879: 0.142028\n",
      "Cost after iteration 1880: 0.141972\n",
      "Cost after iteration 1881: 0.141917\n",
      "Cost after iteration 1882: 0.141861\n",
      "Cost after iteration 1883: 0.141806\n",
      "Cost after iteration 1884: 0.141751\n",
      "Cost after iteration 1885: 0.141696\n",
      "Cost after iteration 1886: 0.141640\n",
      "Cost after iteration 1887: 0.141585\n",
      "Cost after iteration 1888: 0.141530\n",
      "Cost after iteration 1889: 0.141475\n",
      "Cost after iteration 1890: 0.141420\n",
      "Cost after iteration 1891: 0.141365\n",
      "Cost after iteration 1892: 0.141310\n",
      "Cost after iteration 1893: 0.141255\n",
      "Cost after iteration 1894: 0.141200\n",
      "Cost after iteration 1895: 0.141146\n",
      "Cost after iteration 1896: 0.141091\n",
      "Cost after iteration 1897: 0.141036\n",
      "Cost after iteration 1898: 0.140981\n",
      "Cost after iteration 1899: 0.140927\n",
      "Cost after iteration 1900: 0.140872\n",
      "Cost after iteration 1901: 0.140818\n",
      "Cost after iteration 1902: 0.140763\n",
      "Cost after iteration 1903: 0.140708\n",
      "Cost after iteration 1904: 0.140654\n",
      "Cost after iteration 1905: 0.140600\n",
      "Cost after iteration 1906: 0.140545\n",
      "Cost after iteration 1907: 0.140491\n",
      "Cost after iteration 1908: 0.140437\n",
      "Cost after iteration 1909: 0.140382\n",
      "Cost after iteration 1910: 0.140328\n",
      "Cost after iteration 1911: 0.140274\n",
      "Cost after iteration 1912: 0.140220\n",
      "Cost after iteration 1913: 0.140166\n",
      "Cost after iteration 1914: 0.140112\n",
      "Cost after iteration 1915: 0.140058\n",
      "Cost after iteration 1916: 0.140004\n",
      "Cost after iteration 1917: 0.139950\n",
      "Cost after iteration 1918: 0.139896\n",
      "Cost after iteration 1919: 0.139842\n",
      "Cost after iteration 1920: 0.139788\n",
      "Cost after iteration 1921: 0.139734\n",
      "Cost after iteration 1922: 0.139681\n",
      "Cost after iteration 1923: 0.139627\n",
      "Cost after iteration 1924: 0.139573\n",
      "Cost after iteration 1925: 0.139520\n",
      "Cost after iteration 1926: 0.139466\n",
      "Cost after iteration 1927: 0.139413\n",
      "Cost after iteration 1928: 0.139359\n",
      "Cost after iteration 1929: 0.139306\n",
      "Cost after iteration 1930: 0.139252\n",
      "Cost after iteration 1931: 0.139199\n",
      "Cost after iteration 1932: 0.139146\n",
      "Cost after iteration 1933: 0.139092\n",
      "Cost after iteration 1934: 0.139039\n",
      "Cost after iteration 1935: 0.138986\n",
      "Cost after iteration 1936: 0.138933\n",
      "Cost after iteration 1937: 0.138879\n",
      "Cost after iteration 1938: 0.138826\n",
      "Cost after iteration 1939: 0.138773\n",
      "Cost after iteration 1940: 0.138720\n",
      "Cost after iteration 1941: 0.138667\n",
      "Cost after iteration 1942: 0.138614\n",
      "Cost after iteration 1943: 0.138561\n",
      "Cost after iteration 1944: 0.138508\n",
      "Cost after iteration 1945: 0.138456\n",
      "Cost after iteration 1946: 0.138403\n",
      "Cost after iteration 1947: 0.138350\n",
      "Cost after iteration 1948: 0.138297\n",
      "Cost after iteration 1949: 0.138245\n",
      "Cost after iteration 1950: 0.138192\n",
      "Cost after iteration 1951: 0.138139\n",
      "Cost after iteration 1952: 0.138087\n",
      "Cost after iteration 1953: 0.138034\n",
      "Cost after iteration 1954: 0.137982\n",
      "Cost after iteration 1955: 0.137929\n",
      "Cost after iteration 1956: 0.137877\n",
      "Cost after iteration 1957: 0.137825\n",
      "Cost after iteration 1958: 0.137772\n",
      "Cost after iteration 1959: 0.137720\n",
      "Cost after iteration 1960: 0.137668\n",
      "Cost after iteration 1961: 0.137616\n",
      "Cost after iteration 1962: 0.137563\n",
      "Cost after iteration 1963: 0.137511\n",
      "Cost after iteration 1964: 0.137459\n",
      "Cost after iteration 1965: 0.137407\n",
      "Cost after iteration 1966: 0.137355\n",
      "Cost after iteration 1967: 0.137303\n",
      "Cost after iteration 1968: 0.137251\n",
      "Cost after iteration 1969: 0.137199\n",
      "Cost after iteration 1970: 0.137147\n",
      "Cost after iteration 1971: 0.137095\n",
      "Cost after iteration 1972: 0.137044\n",
      "Cost after iteration 1973: 0.136992\n",
      "Cost after iteration 1974: 0.136940\n",
      "Cost after iteration 1975: 0.136888\n",
      "Cost after iteration 1976: 0.136837\n",
      "Cost after iteration 1977: 0.136785\n",
      "Cost after iteration 1978: 0.136734\n",
      "Cost after iteration 1979: 0.136682\n",
      "Cost after iteration 1980: 0.136630\n",
      "Cost after iteration 1981: 0.136579\n",
      "Cost after iteration 1982: 0.136528\n",
      "Cost after iteration 1983: 0.136476\n",
      "Cost after iteration 1984: 0.136425\n",
      "Cost after iteration 1985: 0.136373\n",
      "Cost after iteration 1986: 0.136322\n",
      "Cost after iteration 1987: 0.136271\n",
      "Cost after iteration 1988: 0.136220\n",
      "Cost after iteration 1989: 0.136169\n",
      "Cost after iteration 1990: 0.136117\n",
      "Cost after iteration 1991: 0.136066\n",
      "Cost after iteration 1992: 0.136015\n",
      "Cost after iteration 1993: 0.135964\n",
      "Cost after iteration 1994: 0.135913\n",
      "Cost after iteration 1995: 0.135862\n",
      "Cost after iteration 1996: 0.135811\n",
      "Cost after iteration 1997: 0.135761\n",
      "Cost after iteration 1998: 0.135710\n",
      "Cost after iteration 1999: 0.135659\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you'll build an even better classifier next week!\n",
    "\n",
    "Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0, you predicted that it is a \"cat\" picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "index = 44\n",
    "plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n",
    "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(logistic_regression_model['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Further analysis (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate $\\alpha$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of learning rate ####\n",
    "\n",
    "**Reminder**:\n",
    "In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may \"overshoot\" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.\n",
    "\n",
    "Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print (\"Training a model with learning rate: \" + str(lr))\n",
    "    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- Different learning rates give different costs and thus different predictions results.\n",
    "- If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). \n",
    "- A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.\n",
    "- In deep learning, we usually recommend that you: \n",
    "    - Choose the learning rate that better minimizes the cost function.\n",
    "    - If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:\n",
    "    1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your Coursera Hub.\n",
    "    2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "    3. Change your image's name in the following code\n",
    "    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the name of your image file\n",
    "my_image = \"my_image.jpg\"   \n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "plt.imshow(image)\n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n",
    "\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**What to remember from this assignment:**\n",
    "1. Preprocessing the dataset is important.\n",
    "2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().\n",
    "3. Tuning the learning rate (which is an example of a \"hyperparameter\") can make a big difference to the algorithm. You will see more examples of this later in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:\n",
    "    - Play with the learning rate and the number of iterations\n",
    "    - Try different initialization methods and compare the results\n",
    "    - Test other preprocessings (center the data, or divide each row by its standard deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "- http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/\n",
    "- https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
