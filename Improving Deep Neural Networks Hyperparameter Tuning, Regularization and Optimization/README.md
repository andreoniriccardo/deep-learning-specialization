# [Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network/home/welcome)
## How did this course enhance my Data Science skills?
- I know the importance of splitting the dataset and the correct proportions for train/dev/test sets, depending on the amount of available data
- I know how to detect an overfitting and underfitting model. Moreover, I know how to deal with an overfitting model by implementing different regularization techniques
- I understand the importance of normalizing the input features and of applying batch normalization to each deep network's layer
- I understand the mathematics behind the most used optimization methods (Mini-batch gradient descent, Momentum, RMSprop, Adam), and I know the effect of each hyperparameter
- I know how to perform the hyperparameter tuning process, and how to choose an appropriate scale for each hyperparameter
- I deeply understand the mathematics foundations of Softmax Regression
- I am familiar with the TensorFlow environment and with the `tf.Tensor` objects

## What does this course deal?
- Setting up a Machine Learning Application
	- **Train/development/test set** splitting
	- Detecting **high bias/variance**
- Neural Network Regularization
	- **L2 Regularization**
	- **Dropout Regularization**
- Neural Network optimization problem
	- **Inputs normalization**
	- **Exploding / vanishing gradients**
	- **Weight initialization** for deep networks
	- **Numerical approximation** of the gradients
	- **Gradient Checking**
- Optimization algorithms
	- **Mini-batch** gradient descent
	- **Exponentially Weighted Averages**
	- Gradient descent with **momentum**
	- **RMSprop**
	- **Adam**
	- **Learning rate decay**
- Hyperparameters tuning
	- Picking **hyperparameters' scale**
	- Tuning methods: **pandas vs. caviar**
- Batch Normalization
- Multi-class classification
	- **Softmax** regression
- Programming frameworks
	- **TensorFlow**

## Coding assignments
- **Initialization**: comparing and optimizing weights initialization (random, zeros, He initialization)
- **Regularization**: implementation of regularization techniques to reduce overfitting
- **Gradient Checking**: implementation of gradient checking to verify the accuracy of backpropagation in a fraud detection model
- **Optimization Methods**: implementing and comparing different optimization methods (Stochastic Gradient Descent, Momentum, RMSprop, Adam
- **TensorFlow Introduction**: building a neural network with low level TensorFlow implementation
